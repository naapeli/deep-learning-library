{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Deep Q-Learning Agent for CartPole-v1\n\nThis script implements a Deep Q-Network (DQN) by training an agent to \nbalance a pole in the CartPole-v1 environment from OpenAI Gymnasium. The script also \nimplements a custom training loop of a `DLL.DeepLearning.Model.Model` to train the model.\n\n<img src=\"file://_static/cartpole.gif\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom collections import deque\nimport random\nimport gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nfrom DLL.DeepLearning.Model import Model\nfrom DLL.DeepLearning.Layers import Dense\nfrom DLL.DeepLearning.Layers.Activations import Tanh\nfrom DLL.DeepLearning.Optimisers import RMSPROP, ADAM\nfrom DLL.DeepLearning.Initialisers import Xavier_Normal\n\n\nGAMMA = 0.99\nLR = 0.001\nBATCH_SIZE = 16\nMEMORY_SIZE = 10000\nEPSILON_START = 1.0\nEPSILON_END = 0.01\nEPISODES = 5000  # For optimal results, one should experiment with increasing the amount of episodes in training. About 5000 to 10000 episodes should be sufficient for pretty much optimal agent.\nMOVING_AVERAGE = 50\nMAX_EPISODE_LENGTH = 500\nSAVE_IMG = False\n\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n\n    def add(self, transition):\n        self.buffer.append(transition)\n\n    def sample(self, batch_size):\n        batch = random.sample(self.buffer, batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n        return (\n            torch.stack([torch.tensor(state, dtype=torch.float32) for state in states]),\n            torch.tensor(actions, dtype=torch.long),\n            torch.tensor(rewards, dtype=torch.float32),\n            torch.stack([torch.tensor(state, dtype=torch.float32) for state in next_states]),\n            torch.tensor(dones, dtype=torch.float32)\n        )\n\n    def size(self):\n        return len(self.buffer)\n\ndef epsilon_greedy_policy(state, agent, epsilon, action_space):\n    if random.random() < epsilon:\n        return action_space.sample()\n    else:\n        state = torch.from_numpy(np.array(state, dtype=np.float32)).unsqueeze(0)\n        return agent.predict(state).argmax(dim=1).item()\n\nenv = gym.make(\"CartPole-v1\")\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.n\n\ntorch.manual_seed(1)\n\nhidden_dim = 64\nagent = Model(int(state_dim))\nagent.add(Dense(hidden_dim, activation=Tanh(), initialiser=Xavier_Normal()))\nagent.add(Dense(hidden_dim, activation=Tanh(), initialiser=Xavier_Normal()))\nagent.add(Dense(int(action_dim), initialiser=Xavier_Normal()))\nagent.compile(optimiser=RMSPROP(learning_rate=LR))  # default to MSE loss, The chosen optimiser affects the training results VERY much. ADAM and RMSPROP produce good results, while other optimisers fail to reach the optimum.\nreplay_buffer = ReplayBuffer(MEMORY_SIZE)\n\nepsilon = EPSILON_START\nepsilon_decay = (EPSILON_START - EPSILON_END) / EPISODES\n\nrewards_memory = []\n\nfor episode in range(EPISODES):\n    state = env.reset(seed=episode)[0]\n    total_reward = 0\n\n    step = 0\n    while True:\n        step += 1\n        action = epsilon_greedy_policy(state, agent, epsilon, env.action_space)\n        next_state, reward, done, truncated, info = env.step(action)\n\n        old_state = state\n        state = next_state\n        total_reward += reward\n\n        if done or truncated:\n            if step < MAX_EPISODE_LENGTH:\n                reward = -5  # make the reward lower if the cart fails to balance or goes out-of-bounds.\n            if step == MAX_EPISODE_LENGTH:\n                reward = 5\n            replay_buffer.add((old_state, action, reward, next_state, done or truncated))\n            break\n        \n        # Modify the reward to try to keep the pole in the center. These lines can be removed, however, they make the training faster and more stable.\n        # If these lines are included, the agent does not fail, whereas, if they are not, the agent might make a mistake. The mistake almost always\n        # after collecting 500 total reward, which was considerd to be good enough for the agent to stop the training.\n\n        # Locations\n        reward -= abs(state[0]) / 4.8\n        reward -= abs(state[2]) / 0.418\n\n        # Speeds\n        # reward -= abs(state[1]) / 5\n        # reward -= abs(state[3]) / 5\n\n        replay_buffer.add((old_state, action, reward, next_state, done or truncated))\n\n    epsilon = max(EPSILON_END, epsilon - epsilon_decay)\n\n    # Train the model\n    if replay_buffer.size() >= BATCH_SIZE:\n        states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n\n        agent.optimiser.zero_grad()\n\n        # Compute target Q-values\n        next_q_values = agent.predict(next_states).max(dim=1)[0]\n        targets = rewards + GAMMA * next_q_values * (1 - dones)\n\n        # Compute current Q-values\n        q_values = agent.predict(states, training=True)\n        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n\n        initial_gradient = agent.loss.gradient(q_values, targets)\n        gradient = torch.zeros((BATCH_SIZE, action_dim))\n        gradient[torch.arange(BATCH_SIZE), actions] = initial_gradient  # backpropagate through the q_values.gather line\n        agent.backward(gradient, training=True)\n        agent.optimiser.update_parameters()\n\n    rewards_memory.append(total_reward)\n    print(f\"Episode {episode + 1} | Total Reward: {int(total_reward)} / 500 | Epsilon: {epsilon:.2f}\")\n\n    if sum(rewards_memory[-MOVING_AVERAGE:]) / MOVING_AVERAGE > 490:\n        print(\"Average reward is over 490. The training is stopped.\")\n        break\n\n# plot how the reward changed during training\nplt.plot(rewards_memory)\nplt.plot(np.arange(MOVING_AVERAGE - 1, len(rewards_memory)), np.convolve(rewards_memory, np.ones(MOVING_AVERAGE) / MOVING_AVERAGE, mode='valid'))\nplt.ylabel(\"Total reward\")\nplt.xlabel(\"Episode\")\nplt.show()\n\n\nif SAVE_IMG:\n    # render an example cart\n    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n\n    while input('Write \"stop\" to stop the animation and press enter to rerun: ') != \"stop\":\n        state = env.reset()[0]\n        frames = []\n\n        # Collect frames for animation\n        done = False\n        i = 0\n        while not done:\n            frames.append(env.render())  # Capture frame\n            state = torch.from_numpy(state).unsqueeze(0)\n            best_action = agent.predict(state).argmax(dim=1).item()\n            state, _, done, _, _ = env.step(best_action)\n            if i > 2000:\n                break\n\n        env.close()\n\n        # Create animation\n        fig, ax = plt.subplots()\n        img = ax.imshow(frames[0])\n\n        def update(frame):\n            img.set_array(frames[frame])\n            return img,\n\n        ani = animation.FuncAnimation(fig, update, frames=len(frames), interval=0.1, repeat=False)\n        ani.save(\"./Docs/source/auto_examples/ReinforcementLearning/_static/cartpole.gif\", writer=\"pillow\")\n\n        plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}