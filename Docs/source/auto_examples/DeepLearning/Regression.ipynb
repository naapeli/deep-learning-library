{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Regression with neural networks\n\nThis script implements a model to predict values on a simple quadratic surface. It also \nshowcases some regularisation methods like Dropout and BatchNorm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport matplotlib.pyplot as plt\n\nfrom DLL.DeepLearning.Model import Model\nfrom DLL.DeepLearning.Layers import Dense, Identity, Add\nfrom DLL.DeepLearning.Layers.Regularisation import BatchNorm, Dropout\nfrom DLL.DeepLearning.Layers.Activations import Tanh, Sigmoid, ReLU\nfrom DLL.DeepLearning.Losses import MSE\nfrom DLL.DeepLearning.Optimisers import SGD, RMSPROP\nfrom DLL.DeepLearning.Initialisers import Xavier_Normal, Xavier_Uniform, Kaiming_Normal, Kaiming_Uniform\nfrom DLL.Data.Preprocessing import data_split\n\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nmodel = Model(2, data_type=torch.float32, device=device)\nmodel.add(Dense(6, initialiser=Xavier_Uniform(), normalisation=BatchNorm(), activation=ReLU()))\nmodel.add(Add(Dense(6, activation=ReLU()), Identity(), normalisation=BatchNorm()))\nmodel.add(Dropout(p=0.1))\nmodel.add(Dense(6, initialiser=Kaiming_Normal()))\nmodel.add(BatchNorm())\nmodel.add(Sigmoid())\nmodel.add(Dense(0, initialiser=Xavier_Normal()))\n# model.compile(optimiser=SGD(learning_rate=0.1), loss=MSE(), metrics=[\"loss\", \"val_loss\", \"median_absolute\"])\nmodel.compile(optimiser=RMSPROP(learning_rate=0.01), loss=MSE(), metrics=[\"loss\", \"val_loss\", \"median_absolute\"])\nmodel.summary()\n\nn = 30\nX, Y = torch.meshgrid(torch.linspace(-1, 1, n, dtype=torch.float32, device=device), torch.linspace(-1, 1, n, dtype=torch.float32, device=device), indexing=\"xy\")\nx = torch.stack((X.flatten(), Y.flatten()), dim=1)\ny = X.flatten() ** 2 + Y.flatten() ** 2 + 0.1 * torch.randn(size=Y.flatten().size(), device=device) - 5\nx_train, y_train, x_val, y_val, x_test, y_test = data_split(x, y, train_split=0.6, validation_split=0.2)\n\nerrors = model.fit(x_train, y_train, val_data=(x_val, y_val), epochs=50, batch_size=64, verbose=True)\n\nplt.figure(figsize=(8, 8))\nplt.semilogy(errors[\"loss\"], label=\"loss\")\nplt.semilogy(errors[\"val_loss\"], label=\"val_loss\")\nplt.semilogy(errors[\"median_absolute\"], label=\"median absolute error\")\nplt.legend()\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Metric\")\nz = model.predict(x_test).cpu().numpy()\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\nsurf = ax.scatter(x_test[:, 0].cpu().numpy(), x_test[:, 1].cpu().numpy(), z, color=\"blue\")\nsurf = ax.scatter(x_test[:, 0].cpu().numpy(), x_test[:, 1].cpu().numpy(), y_test.cpu().numpy(), color=\"red\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}