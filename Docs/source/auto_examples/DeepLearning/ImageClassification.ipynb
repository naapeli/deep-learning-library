{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# MNIST Image classification\n\nThis script implements a model to classify the MNIST dataset. The model mainly consists of \nconvolutonal layers and pooling layers with a few dense layers at the end. As the script is \nonly for demonstration purposes, only 100 first datapoints are used to make the training faster. \nFor a full example, change the parameter n to 60000. If n is increased, more epochs may need \nto be added and other hyperparameters tuned.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport matplotlib.pyplot as plt\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor, Compose\n\nfrom DLL.DeepLearning.Model import Model\nfrom DLL.DeepLearning.Layers import Dense, Conv2D, Flatten, MaxPooling2D, Reshape\nfrom DLL.DeepLearning.Layers.Regularisation import Dropout, BatchNorm, GroupNorm, InstanceNorm, LayerNorm\nfrom DLL.DeepLearning.Layers.Activations import ReLU, SoftMax\nfrom DLL.DeepLearning.Losses import CCE\nfrom DLL.DeepLearning.Optimisers import SGD, ADAM\nfrom DLL.DeepLearning.Initialisers import Xavier_Normal, Xavier_Uniform, Kaiming_Normal, Kaiming_Uniform\nfrom DLL.Data.Preprocessing import OneHotEncoder, data_split\nfrom DLL.Data.Metrics import accuracy\n\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\ntransform = Compose([ToTensor()])\ntrain_dataset = MNIST(root=\"./mnist\", train=True, transform=transform, download=True)\ntest_dataset = MNIST(root=\"./mnist\", train=False, transform=transform, download=True)\n\nn = 100  # 60000\ntrain_images = torch.stack([train_dataset[i][0] for i in range(len(train_dataset))])\ntrain_labels = torch.tensor([train_dataset[i][1] for i in range(len(train_dataset))])\ntest_images = torch.stack([test_dataset[i][0] for i in range(len(test_dataset))])\ntest_labels = torch.tensor([test_dataset[i][1] for i in range(len(test_dataset))])\ntrain_images = train_images.to(dtype=torch.float32, device=device)[:n]\ntrain_labels = train_labels.to(dtype=torch.float32, device=device)[:n]\ntest_images = test_images.to(dtype=torch.float32, device=device)[:n]\ntest_labels = test_labels.to(dtype=torch.float32, device=device)[:n]\ntrain_images = train_images / train_images.max()\ntest_images = test_images / test_images.max()\n\ntrain_images, train_labels, validation_images, validation_labels, _, _ = data_split(train_images, train_labels, train_split=0.8, validation_split=0.2)\n\nlabel_encoder = OneHotEncoder()\ntrain_labels = label_encoder.fit_encode(train_labels)\nvalidation_labels = label_encoder.encode(validation_labels)\ntest_labels = label_encoder.encode(test_labels)\nprint(train_images.shape, train_labels.shape, validation_images.shape, validation_labels.shape, test_images.shape, test_labels.shape)\nprint(train_labels[:2])\n\nmodel = Model((1, 28, 28), device=device)\nmodel.add(Conv2D(kernel_size=3, output_depth=32, initialiser=Kaiming_Normal(), activation=ReLU()))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(LayerNorm())\n# model.add(BatchNorm())\nmodel.add(Conv2D(kernel_size=3, output_depth=32, initialiser=Kaiming_Uniform(), activation=ReLU()))\nmodel.add(MaxPooling2D(pool_size=2))\n# model.add(InstanceNorm())\nmodel.add(LayerNorm())\n# model.add(GroupNorm(num_groups=16))\nmodel.add(Dropout(p=0.5))\nmodel.add(Flatten())\n# model.add(Reshape(800))\nmodel.add(Dense(200, activation=ReLU()))\nmodel.add(Dense(10, activation=SoftMax()))\nmodel.compile(optimiser=ADAM(learning_rate=0.001), loss=CCE(), metrics=[\"loss\", \"val_loss\", \"val_accuracy\", \"accuracy\"])\nmodel.summary()\n\nhistory = model.fit(train_images, train_labels, val_data=(validation_images, validation_labels), epochs=25, batch_size=4096, verbose=True)\n\nplt.figure(figsize=(8, 6))\nplt.subplot(1, 2, 1)\nplt.plot(history[\"val_loss\"], label=\"validation loss\")\nplt.plot(history[\"loss\"], label=\"loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Categorical cross entropy\")\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(history[\"val_accuracy\"], label=\"validation accuracy\")\nplt.plot(history[\"accuracy\"], label=\"accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\ntest_pred = model.predict(test_images)\nprint(accuracy(test_pred, test_labels))\nplt.show()\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 8))\nax = ax.ravel()\nfor i in range(len(ax)):\n    ax[i].imshow(test_images[i].numpy()[0], cmap='gray', vmin=0, vmax=1)\n    ax[i].set_title(f\"True label: {test_labels[i].argmax()} | Predicted label: {test_pred[i].argmax()}\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}