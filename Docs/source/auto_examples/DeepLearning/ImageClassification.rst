
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples\DeepLearning\ImageClassification.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_DeepLearning_ImageClassification.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_DeepLearning_ImageClassification.py:


MNIST Image classification
==================================

This script implements a model to classify the MNIST dataset. The model mainly consists of 
convolutonal layers and pooling layers with a few dense layers at the end. As the script is 
only for demonstration purposes, only 100 first datapoints are used to make the training faster. 
For a full example, change the parameter n to 60000. If n is increased, more epochs may need 
to be added and other hyperparameters tuned.

.. GENERATED FROM PYTHON SOURCE LINES 11-97



.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/DeepLearning/images/sphx_glr_ImageClassification_001.png
         :alt: ImageClassification
         :srcset: /auto_examples/DeepLearning/images/sphx_glr_ImageClassification_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/DeepLearning/images/sphx_glr_ImageClassification_002.png
         :alt: True label: 7 | Predicted label: 7, True label: 2 | Predicted label: 0, True label: 1 | Predicted label: 1, True label: 0 | Predicted label: 0
         :srcset: /auto_examples/DeepLearning/images/sphx_glr_ImageClassification_002.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    torch.Size([80, 1, 28, 28]) torch.Size([80, 10]) torch.Size([20, 1, 28, 28]) torch.Size([20, 10]) torch.Size([100, 1, 28, 28]) torch.Size([100, 10])
    tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
            [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])
    Model summary:
    Input - Output: ((1, 28, 28))
    Conv2D - (Input, Output): ((1, 28, 28), (32, 26, 26)) - Parameters: 21920
        ReLU - Output: ((32, 26, 26))
    MaxPooling2D - Output: ((32, 13, 13))
    Layer normalisation - Output: ((32, 13, 13)) - Parameters: 64
    Conv2D - (Input, Output): ((32, 13, 13), (32, 11, 11)) - Parameters: 13088
        ReLU - Output: ((32, 11, 11))
    MaxPooling2D - Output: ((32, 5, 5))
    Layer normalisation - Output: ((32, 5, 5)) - Parameters: 64
    Dropout - Output: ((32, 5, 5)) - Keep probability: 0.5
    Flatten - (Input, Output): ((32, 5, 5), 800)
    Dense - (Input, Output): (800, 200) - Parameters: 160200
        ReLU - Output: (200)
    Dense - (Input, Output): (200, 10) - Parameters: 2010
        Softmax - Output: (10)
    Total number of parameters: 197346
    Epoch: 1 - Metrics: {'loss': '2.0483', 'accuracy': '0.2625', 'val_loss': '2.3640', 'val_accuracy': '0.1500'}
    Epoch: 2 - Metrics: {'loss': '1.7642', 'accuracy': '0.3375', 'val_loss': '2.2486', 'val_accuracy': '0.2500'}
    Epoch: 3 - Metrics: {'loss': '1.4590', 'accuracy': '0.6125', 'val_loss': '2.1096', 'val_accuracy': '0.3500'}
    Epoch: 4 - Metrics: {'loss': '1.2577', 'accuracy': '0.7125', 'val_loss': '1.9706', 'val_accuracy': '0.3000'}
    Epoch: 5 - Metrics: {'loss': '1.0529', 'accuracy': '0.7625', 'val_loss': '1.7798', 'val_accuracy': '0.4000'}
    Epoch: 6 - Metrics: {'loss': '0.8848', 'accuracy': '0.8000', 'val_loss': '1.6372', 'val_accuracy': '0.4500'}
    Epoch: 7 - Metrics: {'loss': '0.7328', 'accuracy': '0.8625', 'val_loss': '1.4809', 'val_accuracy': '0.4500'}
    Epoch: 8 - Metrics: {'loss': '0.6055', 'accuracy': '0.8875', 'val_loss': '1.3601', 'val_accuracy': '0.4500'}
    Epoch: 9 - Metrics: {'loss': '0.4819', 'accuracy': '0.9500', 'val_loss': '1.2351', 'val_accuracy': '0.5000'}
    Epoch: 10 - Metrics: {'loss': '0.3835', 'accuracy': '0.9500', 'val_loss': '1.1514', 'val_accuracy': '0.6000'}
    Epoch: 11 - Metrics: {'loss': '0.3022', 'accuracy': '0.9500', 'val_loss': '1.0922', 'val_accuracy': '0.7000'}
    Epoch: 12 - Metrics: {'loss': '0.2449', 'accuracy': '0.9625', 'val_loss': '1.0510', 'val_accuracy': '0.6500'}
    Epoch: 13 - Metrics: {'loss': '0.2069', 'accuracy': '0.9625', 'val_loss': '1.0279', 'val_accuracy': '0.6500'}
    Epoch: 14 - Metrics: {'loss': '0.1781', 'accuracy': '0.9750', 'val_loss': '1.0142', 'val_accuracy': '0.6500'}
    Epoch: 15 - Metrics: {'loss': '0.1516', 'accuracy': '0.9750', 'val_loss': '0.9910', 'val_accuracy': '0.6500'}
    Epoch: 16 - Metrics: {'loss': '0.1252', 'accuracy': '0.9875', 'val_loss': '0.9315', 'val_accuracy': '0.6500'}
    Epoch: 17 - Metrics: {'loss': '0.1059', 'accuracy': '0.9875', 'val_loss': '0.8825', 'val_accuracy': '0.6500'}
    Epoch: 18 - Metrics: {'loss': '0.0904', 'accuracy': '0.9875', 'val_loss': '0.8378', 'val_accuracy': '0.7000'}
    Epoch: 19 - Metrics: {'loss': '0.0770', 'accuracy': '1.0000', 'val_loss': '0.8065', 'val_accuracy': '0.7000'}
    Epoch: 20 - Metrics: {'loss': '0.0664', 'accuracy': '1.0000', 'val_loss': '0.7899', 'val_accuracy': '0.6500'}
    Epoch: 21 - Metrics: {'loss': '0.0576', 'accuracy': '1.0000', 'val_loss': '0.7888', 'val_accuracy': '0.6500'}
    Epoch: 22 - Metrics: {'loss': '0.0504', 'accuracy': '1.0000', 'val_loss': '0.7922', 'val_accuracy': '0.6500'}
    Epoch: 23 - Metrics: {'loss': '0.0436', 'accuracy': '1.0000', 'val_loss': '0.7958', 'val_accuracy': '0.6500'}
    Epoch: 24 - Metrics: {'loss': '0.0373', 'accuracy': '1.0000', 'val_loss': '0.7962', 'val_accuracy': '0.6500'}
    Epoch: 25 - Metrics: {'loss': '0.0322', 'accuracy': '1.0000', 'val_loss': '0.8012', 'val_accuracy': '0.6000'}
    0.7400000095367432






|

.. code-block:: Python

    import torch
    import matplotlib.pyplot as plt
    from torchvision.datasets import MNIST
    from torchvision.transforms import ToTensor, Compose

    from DLL.DeepLearning.Model import Model
    from DLL.DeepLearning.Layers import Dense, Conv2D, Flatten, MaxPooling2D, Reshape
    from DLL.DeepLearning.Layers.Regularisation import Dropout, BatchNorm, GroupNorm, InstanceNorm, LayerNorm
    from DLL.DeepLearning.Layers.Activations import ReLU, SoftMax
    from DLL.DeepLearning.Losses import CCE
    from DLL.DeepLearning.Optimisers import SGD, ADAM
    from DLL.DeepLearning.Initialisers import Xavier_Normal, Xavier_Uniform, Kaiming_Normal, Kaiming_Uniform
    from DLL.Data.Preprocessing import OneHotEncoder, data_split
    from DLL.Data.Metrics import accuracy


    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

    transform = Compose([ToTensor()])
    train_dataset = MNIST(root="./mnist", train=True, transform=transform, download=True)
    test_dataset = MNIST(root="./mnist", train=False, transform=transform, download=True)

    n = 100  # 60000
    train_images = torch.stack([train_dataset[i][0] for i in range(len(train_dataset))])
    train_labels = torch.tensor([train_dataset[i][1] for i in range(len(train_dataset))])
    test_images = torch.stack([test_dataset[i][0] for i in range(len(test_dataset))])
    test_labels = torch.tensor([test_dataset[i][1] for i in range(len(test_dataset))])
    train_images = train_images.to(dtype=torch.float32, device=device)[:n]
    train_labels = train_labels.to(dtype=torch.float32, device=device)[:n]
    test_images = test_images.to(dtype=torch.float32, device=device)[:n]
    test_labels = test_labels.to(dtype=torch.float32, device=device)[:n]
    train_images = train_images / train_images.max()
    test_images = test_images / test_images.max()

    train_images, train_labels, validation_images, validation_labels, _, _ = data_split(train_images, train_labels, train_split=0.8, validation_split=0.2)

    label_encoder = OneHotEncoder()
    train_labels = label_encoder.fit_encode(train_labels)
    validation_labels = label_encoder.encode(validation_labels)
    test_labels = label_encoder.encode(test_labels)
    print(train_images.shape, train_labels.shape, validation_images.shape, validation_labels.shape, test_images.shape, test_labels.shape)
    print(train_labels[:2])

    model = Model((1, 28, 28), device=device)
    model.add(Conv2D(kernel_size=3, output_depth=32, initialiser=Kaiming_Normal(), activation=ReLU()))
    model.add(MaxPooling2D(pool_size=2))
    model.add(LayerNorm())
    # model.add(BatchNorm())
    model.add(Conv2D(kernel_size=3, output_depth=32, initialiser=Kaiming_Uniform(), activation=ReLU()))
    model.add(MaxPooling2D(pool_size=2))
    # model.add(InstanceNorm())
    model.add(LayerNorm())
    # model.add(GroupNorm(num_groups=16))
    model.add(Dropout(p=0.5))
    model.add(Flatten())
    # model.add(Reshape(800))
    model.add(Dense(200, activation=ReLU()))
    model.add(Dense(10, activation=SoftMax()))
    model.compile(optimiser=ADAM(learning_rate=0.001), loss=CCE(), metrics=["loss", "val_loss", "val_accuracy", "accuracy"])
    model.summary()

    history = model.fit(train_images, train_labels, val_data=(validation_images, validation_labels), epochs=25, batch_size=4096, verbose=True)

    plt.figure(figsize=(8, 6))
    plt.subplot(1, 2, 1)
    plt.plot(history["val_loss"], label="validation loss")
    plt.plot(history["loss"], label="loss")
    plt.xlabel("Epoch")
    plt.ylabel("Categorical cross entropy")
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(history["val_accuracy"], label="validation accuracy")
    plt.plot(history["accuracy"], label="accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend()
    test_pred = model.predict(test_images)
    print(accuracy(test_pred, test_labels))
    plt.show()

    fig, ax = plt.subplots(2, 2, figsize=(8, 8))
    ax = ax.ravel()
    for i in range(len(ax)):
        ax[i].imshow(test_images[i].numpy()[0], cmap='gray', vmin=0, vmax=1)
        ax[i].set_title(f"True label: {test_labels[i].argmax()} | Predicted label: {test_pred[i].argmax()}")
    plt.show()


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 8.710 seconds)


.. _sphx_glr_download_auto_examples_DeepLearning_ImageClassification.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: ImageClassification.ipynb <ImageClassification.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: ImageClassification.py <ImageClassification.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: ImageClassification.zip <ImageClassification.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
