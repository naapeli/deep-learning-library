{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Gaussian Process Regressor (GPR)\n\nThis script demonstrates the use of a custom Gaussian Process Regressor (GPR) \nmodel with a compound kernel on generated data. The model is trained using \na combination of a linear kernel and a periodic kernel, and the training \nprocess optimizes the kernel parameters to fit the data. The script also \ncompares the custom GPR model with the GPR implementation from Scikit-learn \nusing a different kernel combination.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport matplotlib.pyplot as plt\nfrom sklearn.gaussian_process import GaussianProcessRegressor as GPR\nfrom sklearn.gaussian_process.kernels import DotProduct, ExpSineSquared, ConstantKernel, Matern as sk_matern\n\nfrom DLL.MachineLearning.SupervisedLearning.GaussianProcesses import GaussianProcessRegressor\nfrom DLL.MachineLearning.SupervisedLearning.Kernels import RBF, Linear, WhiteGaussian, Periodic, RationalQuadratic, Matern\nfrom DLL.DeepLearning.Optimisers import ADAM, LBFGS\nfrom DLL.Data.Preprocessing import StandardScaler\n\n\n# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\ndevice = torch.device(\"cpu\")\nX = torch.linspace(0, 1, 20, dtype=torch.float64, device=device).unsqueeze(1)\nY = torch.sin(3 * 2 * torch.pi * X) + 3 * X ** 2# + torch.randn_like(X) * 0.5\ntransformer = StandardScaler()\nY = transformer.fit_transform(Y).squeeze(dim=1)\n\ntrain_kernel = True  # try to changing this line of code to see how the covariance kernel learns the correct parameters\n\nmodel = GaussianProcessRegressor(Linear(sigma=0.2, sigma_bias=1) ** 2 + Periodic(1, 2, period=0.5), noise=0.1, device=device)\nsk_model = GPR(ConstantKernel(constant_value=0.2) * DotProduct(sigma_0=1) ** 2 + ExpSineSquared())\n# correlation_length = 0.1\n# nu = 0.5\n# model = GaussianProcessRegressor(Matern(sigma=2, correlation_length=correlation_length, nu=nu), noise=0.0, device=device)\n# sk_model = GPR(sk_matern(nu=nu, length_scale=correlation_length))\nmodel.fit(X, Y)\nprint(model.log_marginal_likelihood())\nif train_kernel:\n    # history = model.train_kernel(epochs=2000, optimiser=Adam(), verbose=True)\n    history = model.train_kernel(epochs=30, optimiser=LBFGS(model.log_marginal_likelihood, learning_rate=0.1), verbose=True)\n    plt.plot(history[\"log marginal likelihood\"])\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"log marginal likelihood\")\n    plt.title(\"The change in log marginal likelihood during kernel training\")\n\nsk_model.fit(X, Y)\nprint(sk_model.kernel_.get_params())\n\nx_test = torch.linspace(0, 2, 100, dtype=torch.float64, device=device).unsqueeze(1)\nmean, covariance = model.predict(x_test)\nmean = mean.squeeze()\nmean = transformer.inverse_transform(mean)\ncovariance = covariance * transformer.var ** 2\nstd = torch.sqrt(torch.diag(covariance))\n\nplt.figure()\nplt.plot(X.cpu(), transformer.inverse_transform(Y).cpu(), \".\")\nplt.plot(x_test.cpu(), mean.cpu(), color=\"blue\", label=\"mean\")\nplt.plot(x_test.cpu(), transformer.inverse_transform(torch.from_numpy(sk_model.predict(x_test.numpy()))), color=\"lightblue\", label=\"sklearn implementation\")\nplt.fill_between(x_test.squeeze(dim=1).cpu(), mean.cpu() - 1.96 * std.cpu(), mean.cpu() + 1.96 * std.cpu(), alpha=0.1, color=\"blue\", label=r\"95% cofidence interval\")\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Predictions with the GPR model\")\n\n# draw random samples from the distribution\nblue_theme = [\n    \"#1f77b4\",  # blue\n    \"#4a8cd3\",  # lighter blue\n    \"#005cbf\",  # dark blue\n    \"#7cb9e8\",  # sky blue\n    \"#0073e6\",  # vivid blue\n    \"#3b5998\",  # muted blue\n]\nplt.rcParams[\"axes.prop_cycle\"] = plt.cycler(color=blue_theme)\n\n\ntry:\n    distribution = torch.distributions.MultivariateNormal(mean, covariance)\n    plt.figure()\n    plt.plot(X.cpu(), transformer.inverse_transform(Y).cpu(), \".\")\n    for _ in range(5):\n        y = distribution.sample()\n        plt.plot(x_test.cpu(), y.cpu())\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Random samples from the previous distribution\")\nexcept:\n    pass\nfinally:\n    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}