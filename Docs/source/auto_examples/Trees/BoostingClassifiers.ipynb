{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Boosting Classifier Comparison\n\nThis script compares the performance of various boosting classifiers, including \nGradient Boosting, AdaBoost, XGBoost, and LightGBM. Also sklearns version of AdaBoost \nis compared.\n\nThe script generates a synthetic dataset, trains multiple models, measures execution time, \ncomputes classification accuracy, and visualizes loss curves and ROC curves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import AdaBoostClassifier as sk_AdaBoostClassifier, GradientBoostingClassifier as sk_GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom time import perf_counter\n\nfrom DLL.MachineLearning.SupervisedLearning.Trees import GradientBoostingClassifier, AdaBoostClassifier, XGBoostingClassifier, LGBMClassifier\nfrom DLL.Data.Preprocessing import data_split\nfrom DLL.Data.Metrics import accuracy, roc_curve, auc\n\nn_classes = 2\nX, y = datasets.make_blobs(n_samples=200, n_features=2, cluster_std=3, centers=n_classes, random_state=3)\n\nx_train, y_train, _, _, x_test, y_test = data_split(torch.from_numpy(X).to(dtype=torch.float32), torch.from_numpy(y), train_split=0.7, validation_split=0.0)\n\nmodel = GradientBoostingClassifier(n_trees=50, max_depth=1, learning_rate=0.5, loss=\"log_loss\")\nstart = perf_counter()\nhistory = model.fit(x_train, y_train)\nprint(f\"GBM time with {model.n_trees} weak learners: {perf_counter() - start:.2f} seconds\")\ny_pred_proba = model.predict_proba(x_test)\ny_pred = model.predict(x_test)\nprint(\"gradientboost accuracy: \", accuracy(y_pred, y_test))\n\nmodel2 = AdaBoostClassifier(n_trees=50, max_depth=1)\nstart = perf_counter()\nerrors = model2.fit(x_train, y_train, verbose=False)\nprint(f\"Adaboost time with {model2.n_trees} weak learners: {perf_counter() - start:.2f} seconds\")\ny_pred_proba2 = model2.predict_proba(x_test)\ny_pred2 = model2.predict(x_test)\nprint(\"adaboost accuracy: \", accuracy(y_pred2, y_test))\n\nmodel3 = sk_AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), learning_rate=0.5)\n# model3 = sk_GradientBoostingClassifier()\nstart = perf_counter()\nmodel3.fit(x_train.numpy(), y_train.numpy())\nprint(f\"SKlearn adaboost time with 50 weak learners: {perf_counter() - start:.2f} seconds\")\npred3 = torch.from_numpy(model3.predict(x_test.numpy()))\nprint(\"sklearn accuracy: \", accuracy(pred3, y_test))\n\nmodel4 = XGBoostingClassifier(n_trees=50, learning_rate=0.5, reg_lambda=0.01, max_depth=1, loss=\"log_loss\")\nstart = perf_counter()\nhistory4 = model4.fit(x_train, y_train)\nprint(f\"XGBoost time with {model4.n_trees} weak learners: {perf_counter() - start:.2f} seconds\")\ny_pred_proba4 = model4.predict_proba(x_test)\ny_pred4 = model4.predict(x_test)\nprint(\"XGBoost accuracy: \", accuracy(y_pred4, y_test))\n\nmodel5 = LGBMClassifier(n_trees=200, learning_rate=0.2, reg_lambda=0.01, max_depth=1, loss=\"log_loss\", large_error_proportion=0.5, small_error_proportion=0.2)\nstart = perf_counter()\nhistory5 = model5.fit(x_train, y_train)\nprint(f\"LGBM time with {model5.n_trees} weak learners: {perf_counter() - start:.2f} seconds\")\ny_pred_proba5 = model5.predict_proba(x_test)\ny_pred5 = model5.predict(x_test)\nprint(\"LGBM accuracy: \", accuracy(y_pred5, y_test))\n\nplt.title(\"Ada boost errors and alphas\")\nplt.plot(errors, label=\"errors\")\nplt.plot(model2.confidences, label=\"confidences\")\nplt.legend()\n\nif n_classes == 2:\n    plt.figure()\n    plt.plot(history[\"loss\"])\n    plt.ylabel(\"Loss\")\n    plt.xlabel(\"Tree\")\n    plt.title(\"Gradient boosting classifier loss as a function of fitted trees\")\n    \n    plt.figure()\n    plt.plot(history4[\"loss\"])\n    plt.ylabel(\"Loss\")\n    plt.xlabel(\"Tree\")\n    plt.title(\"XGBoost loss as a function of fitted trees\")\n\n    plt.figure()\n    plt.plot(history5[\"loss\"])\n    plt.ylabel(\"Loss\")\n    plt.xlabel(\"Tree\")\n    plt.title(\"LGBM loss as a function of fitted trees\")\n\n    thresholds = torch.linspace(0, 1, 100)\n    fpr, tpr = roc_curve(y_pred_proba, y_test, thresholds)\n    fpr2, tpr2 = roc_curve(y_pred_proba2, y_test, thresholds)\n    fpr4, tpr4 = roc_curve(y_pred_proba4, y_test, thresholds)\n    fpr5, tpr5 = roc_curve(y_pred_proba5, y_test, thresholds)\n    fig, ax = plt.subplots(2, 2, figsize=(8, 8))\n    plt.subplots_adjust(hspace=0.5)\n    ax = ax.ravel()\n    ax[0].set_title(f\"gradient boosting ROC | auc = {auc(fpr, tpr):.3f}\")\n    ax[0].plot([0, 1], [0, 1])\n    ax[0].step(fpr, tpr)\n    ax[0].set_xlabel(\"False positive rate\")\n    ax[0].set_ylabel(\"True positive rate\")\n\n    ax[1].set_title(f\"ada boost ROC | auc = {auc(fpr2, tpr2):.3f}\")\n    ax[1].plot([0, 1], [0, 1])\n    ax[1].step(fpr2, tpr2)\n    ax[1].set_xlabel(\"False positive rate\")\n    ax[1].set_ylabel(\"True positive rate\")\n\n    ax[2].set_title(f\"XG boost ROC | auc = {auc(fpr4, tpr4):.3f}\")\n    ax[2].plot([0, 1], [0, 1])\n    ax[2].step(fpr4, tpr4)\n    ax[2].set_xlabel(\"False positive rate\")\n    ax[2].set_ylabel(\"True positive rate\")\n\n    ax[3].set_title(f\"LGBM ROC | auc = {auc(fpr5, tpr5):.3f}\")\n    ax[3].plot([0, 1], [0, 1])\n    ax[3].step(fpr5, tpr5)\n    ax[3].set_xlabel(\"False positive rate\")\n    ax[3].set_ylabel(\"True positive rate\")\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 8))\nplt.subplots_adjust(hspace=0.5)\nax = ax.ravel()\nax[0].scatter(x_test[:, 0], x_test[:, 1], c=y_test, alpha=((model.predict(x_test) != y_test) + 0.2) / 1.2)\nax[0].set_title(\"Gradient boosting\")\nax[1].scatter(x_test[:, 0], x_test[:, 1], c=y_test, alpha=((model2.predict(x_test) != y_test) + 0.2) / 1.2)\nax[1].set_title(\"Adaptive boosting\")\nax[2].scatter(x_test[:, 0], x_test[:, 1], c=y_test, alpha=((model4.predict(x_test) != y_test) + 0.2) / 1.2)\nax[2].set_title(\"XG boosting\")\nax[3].scatter(x_test[:, 0], x_test[:, 1], c=y_test, alpha=((model5.predict(x_test) != y_test) + 0.2) / 1.2)\nax[3].set_title(\"Light gradient boosting\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}