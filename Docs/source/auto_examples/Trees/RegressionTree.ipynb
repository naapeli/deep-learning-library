{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Regression using tree based models\n\nThis script evaluates and compares various regression models, including regression trees, random forest, gradient boosting, AdaBoost, XGBoost, and LGBM, using synthetic datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import GradientBoostingRegressor as gbr, AdaBoostRegressor as abr\nfrom sklearn.tree import DecisionTreeRegressor\nfrom time import perf_counter\n\nfrom DLL.MachineLearning.SupervisedLearning.Trees import RegressionTree, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, XGBoostingRegressor, LGBMRegressor\nfrom DLL.Data.Preprocessing import data_split\n\n\nn = 100\nx = torch.linspace(0, 1, n).unsqueeze(-1)\ny = 0.2 * torch.sin(20 * x) + x * x + torch.normal(mean=0, std=0.05, size=(n, 1))\ny = y.squeeze()\nrand_feats = torch.randint_like(x, 2)\nx = torch.cat([x, rand_feats], dim=1)\n\nmodel = RegressionTree()\nmodel.fit(x, y)\nx_test, _ = torch.rand((n, 1)).sort(dim=0)\nx_test_rand_feats = torch.randint_like(x_test, 2)\nx_test = torch.cat([x_test, x_test_rand_feats], dim=1)\ny_pred = model.predict(x_test)\n\nmodel2 = RandomForestRegressor(n_trees=3)\nmodel2.fit(x, y)\ny_pred2 = model2.predict(x_test)\n\nmodel3 = GradientBoostingRegressor(n_trees=50, learning_rate=0.05, loss=\"absolute\", max_depth=3)\nhistory3 = model3.fit(x, y, metrics=[\"loss\"])\ny_pred3 = model3.predict(x_test)\n\nmodel4 = gbr(n_estimators=10, learning_rate=0.5, loss=\"absolute_error\")\nmodel4.fit(x, y.ravel())\ny_pred4 = model4.predict(x_test)\n\nloss_adaboost = \"exponential\"\nmodel5 = AdaBoostRegressor(n_trees=50, loss=loss_adaboost, max_depth=3)\nerrors5 = model5.fit(x, y)\ny_pred5 = model5.predict(x_test)\n\nmodel6 = abr(estimator=DecisionTreeRegressor(max_depth=3), n_estimators=50, loss=loss_adaboost)\nmodel6.fit(x, y.ravel())\ny_pred6 = model6.predict(x_test)\n\nmodel7 = XGBoostingRegressor(n_trees=50, learning_rate=0.2, loss=\"huber\", max_depth=3, reg_lambda=0.01, gamma=0, huber_delta=5)\nstart = perf_counter()\nhistory7 = model7.fit(x, y, metrics=[\"loss\"])\nprint(f\"XGBoost time with {model7.n_trees} weak learners: {perf_counter() - start}\")\ny_pred7 = model7.predict(x_test)\n\nmodel8 = LGBMRegressor(n_trees=150, learning_rate=0.2, loss=\"squared\", max_depth=3, reg_lambda=0.01, gamma=0, huber_delta=5, large_error_proportion=0.3, small_error_proportion=0.2)\nstart = perf_counter()\nhistory8 = model8.fit(x, y, metrics=[\"loss\"])\nprint(f\"LGBM time with {model8.n_trees} weak learners: {perf_counter() - start}\")\ny_pred8 = model8.predict(x_test)\n\nplt.figure(figsize=(8, 8))\nplt.plot(x[:, 0].numpy(), y.numpy(), color=\"Blue\", label=\"True data\")\nplt.plot(x_test[:, 0].numpy(), y_pred.numpy(), color=\"Red\", label=\"Regression tree\")\nplt.plot(x_test[:, 0].numpy(), y_pred2.numpy(), color=\"Green\", label=\"Random forest regressor\")\nplt.plot(x_test[:, 0].numpy(), y_pred3.numpy(), color=\"Yellow\", label=\"GBR\")\nplt.plot(x_test[:, 0].numpy(), y_pred4, color=\"gray\", label=\"SKlearn GBR\")\nplt.plot(x_test[:, 0].numpy(), y_pred5.numpy(), color=\"brown\", label=\"AdaBoostRegressor\")\nplt.plot(x_test[:, 0].numpy(), y_pred6, color=\"pink\", label=\"SKlearn AdaBoostRegressor\")\nplt.plot(x_test[:, 0].numpy(), y_pred7.numpy(), color=\"lightblue\", label=\"XGBoostRegressor\")\nplt.plot(x_test[:, 0].numpy(), y_pred8.numpy(), color=\"black\", label=\"LGBMRegressor\")\nplt.legend(loc=\"upper left\")\nplt.show()\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 8))\nplt.subplots_adjust(hspace=0.5, wspace=0.5)\nax = ax.ravel()\nax[0].plot(history3[\"loss\"])\nax[0].set_ylabel(\"Loss\")\nax[0].set_xlabel(\"Tree\")\nax[0].set_title(\"Gradient boosting regressor\")\nax[1].plot(errors5, label=\"errors\")\nax[1].plot(model5.confidences, label=\"confidences\")\nax[1].set_ylabel(\"AdaBoost error\")\nax[1].set_xlabel(\"Tree\")\nax[1].set_title(\"AdaBoost\")\nax[1].legend()\nax[2].plot(history7[\"loss\"])\nax[2].set_ylabel(\"Loss\")\nax[2].set_xlabel(\"Tree\")\nax[2].set_title(\"Extreme gradient boosting regressor\")\nax[3].plot(history8[\"loss\"])\nax[3].set_ylabel(\"Loss\")\nax[3].set_xlabel(\"Tree\")\nax[3].set_title(\"Light gradient boosting machine regressor\")\nplt.show()\n\nn = 20\nX, Y = torch.meshgrid(torch.linspace(-1, 1, n, dtype=torch.float32), torch.linspace(-1, 1, n, dtype=torch.float32), indexing=\"xy\")\nx = torch.stack((X.flatten(), Y.flatten()), dim=1)\ny = X.flatten() ** 2 + Y.flatten() ** 2 + 0.1 * torch.randn(size=Y.flatten().size()) - 5\nx_train, y_train, _, _, x_test, y_test = data_split(x, y, train_split=0.8, validation_split=0.0)\n\nmodel.fit(x_train, y_train)\nz1 = model.predict(x_test)\n\nmodel2.fit(x_train, y_train)\nz2 = model2.predict(x_test)\n\nhistory3 = model3.fit(x_train, y_train)\nz3 = model3.predict(x_test)\n\nmodel4.fit(x_train, y_train)\nz4 = model4.predict(x_test)\n\nerrors5 = model5.fit(x_train, y_train)\nz5 = model5.predict(x_test)\n\nmodel6.fit(x_train, y_train)\nz6 = model6.predict(x_test)\n\nhistory7 = model7.fit(x_train, y_train)\nz7 = model7.predict(x_test)\n\nhistory8 = model8.fit(x_train, y_train)\nz8 = model8.predict(x_test)\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x_test[:, 0], x_test[:, 1], y_test, color=\"Blue\", label=\"True data\")\nax.scatter(x_test[:, 0], x_test[:, 1], z1, color=\"Red\", label=\"Regression tree\")\nax.scatter(x_test[:, 0], x_test[:, 1], z2, color=\"Green\", label=\"Random forest regressor\")\nax.scatter(x_test[:, 0], x_test[:, 1], z3, color=\"Yellow\", label=\"GBR\")\nax.scatter(x_test[:, 0], x_test[:, 1], z4, color=\"gray\", label=\"SKlearn GBR\")\nax.scatter(x_test[:, 0], x_test[:, 1], z5, color=\"brown\", label=\"AdaBoostRegressor\")\nax.scatter(x_test[:, 0], x_test[:, 1], z6, color=\"pink\", label=\"SKlearn AdaBoostRegressor\")\nax.scatter(x_test[:, 0], x_test[:, 1], z7, color=\"lightblue\", label=\"XGBoostRegressor\")\nax.scatter(x_test[:, 0], x_test[:, 1], z8, color=\"black\", label=\"LGBMRegressor\")\nplt.legend(loc=\"upper left\")\nplt.show()\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 8))\nplt.subplots_adjust(hspace=0.5, wspace=0.5)\nax = ax.ravel()\nax[0].plot(history3[\"loss\"])\nax[0].set_ylabel(\"Loss\")\nax[0].set_xlabel(\"Tree\")\nax[0].set_title(\"Gradient boosting regressor\")\nax[1].plot(errors5, label=\"errors\")\nax[1].plot(model5.confidences, label=\"confidences\")\nax[1].set_ylabel(\"AdaBoost error\")\nax[1].set_xlabel(\"Tree\")\nax[1].set_title(\"AdaBoost\")\nax[1].legend()\nax[2].plot(history7[\"loss\"])\nax[2].set_ylabel(\"Loss\")\nax[2].set_xlabel(\"Tree\")\nax[2].set_title(\"Extreme gradient boosting regressor\")\nax[3].plot(history8[\"loss\"])\nax[3].set_ylabel(\"Loss\")\nax[3].set_xlabel(\"Tree\")\nax[3].set_title(\"Light gradient boosting machine regressor\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}