{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Kolmogorov-Arnold Networks\n\nThis script implements a model using Kolmogorov-Arnold networks. It fits to a simple \nquadratic surface using only a few parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport matplotlib.pyplot as plt\n\nfrom DLL.DeepLearning.Layers._DenseKAN import _get_basis_functions, _NeuronKAN\nfrom DLL.DeepLearning.Layers import DenseKAN\nfrom DLL.DeepLearning.Layers.Activations import Tanh\nfrom DLL.DeepLearning.Model import Model\nfrom DLL.DeepLearning.Optimisers import ADAM\nfrom DLL.DeepLearning.Losses import MSE\nfrom DLL.DeepLearning.Initialisers import Xavier_Normal\nfrom DLL.Data.Preprocessing import data_split\n\n\n# X = torch.linspace(-1, 1, 100).unsqueeze(1)\n# y = torch.sin(4 * X).squeeze()\n# X = torch.linspace(-1, 1, 50).unsqueeze(1)\n# y = (0.5 * torch.sin(4 * X) * torch.exp(-X - 1) + 0.5).squeeze()\nn = 30\nX, Y = torch.meshgrid(torch.linspace(-1, 1, n), torch.linspace(-1, 1, n), indexing=\"xy\")\nx = torch.stack((X.flatten(), Y.flatten()), dim=1)\ny = X.flatten() ** 2 + Y.flatten() ** 2 + 0.1 * torch.randn(size=Y.flatten().size()) - 5\nX, y, _, _, X_test, y_test = data_split(x, y, train_split=0.8, validation_split=0.0)\n\n\nmodel = Model(2)\n# model.add(DenseKAN(1, activation=Tanh(), initialiser=Xavier_Normal()))\nmodel.add(DenseKAN(2, activation=Tanh(), initialiser=Xavier_Normal()))\n# model.add(DenseKAN(2, activation=Tanh(), initialiser=Xavier_Normal()))\nmodel.add(DenseKAN(0, initialiser=Xavier_Normal()))\nmodel.compile(ADAM(0.01), MSE())\nmodel.summary()\n\nhistory = model.fit(X, y, epochs=300, verbose=True)\n\n# X_test = 2 * torch.rand_like(X) - 1\n# y_test = torch.sin(4 * X_test).squeeze()\n# X_test = 2 * torch.rand_like(X) - 1\n# y_test = (0.5 * torch.sin(4 * X_test) * torch.exp(-X_test - 1) + 0.5).squeeze()\n\n# plt.figure()\n# plt.scatter(X_test, y_test, label=\"True test points\")\n# plt.scatter(X_test, model.predict(X_test), label=\"Predictions\")\n# plt.scatter(X, y, label=\"True train points\")\n# plt.scatter(X, model.predict(X), label=\"Predicted train points\")\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_test[:, 0], X_test[:, 1], y_test, label=\"True test points\")\nax.scatter(X_test[:, 0], X_test[:, 1], model.predict(X_test), label=\"Predictions\")\nplt.legend()\n\n\nplt.figure(figsize=(8, 8))\nplt.plot(history[\"loss\"])\nplt.title(\"Loss function as a function of epoch\")\nplt.ylabel(\"MSE\")\nplt.xlabel(\"Epoch\")\nplt.show()\n\nn_fun = 10\nbasis_funcs, basis_func_derivatives = _get_basis_functions(n_fun, degree=5, bounds=(-1, 1))\nx = torch.linspace(-1, 1, 100).unsqueeze(1)\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 6))\nplt.subplots_adjust(wspace=0.3)\nfor i in range(n_fun):\n    basis_values = basis_funcs[i](x)\n    derivative_values = basis_func_derivatives[i](x)\n\n    ax[0].plot(x.squeeze(1), basis_values)\n    ax[1].plot(x, derivative_values)\n\nax[0].set_title(\"B-spline Basis Functions\")\nax[0].set_xlabel(\"x\")\nax[0].set_ylabel(\"y\")\nax[0].grid()\nax[1].set_title(\"B-spline Basis Function Derivatives\")\nax[1].set_xlabel(\"x\")\nax[1].set_ylabel(\"y\")\nax[1].grid()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}