{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison of Optimization Algorithms on the Rosenbrock Function\n\nThis script demonstrates the performance of various optimization algorithms in minimizing the \nRosenbrock function, a well-known test problem in optimization. \n\n**Optimization Algorithms from DLL.DeepLearning.Optimisers**:\n    - **LBFGS** (Limited-memory BFGS)\n    - **SGD** (Stochastic Gradient Descent)\n    - **ADAM** (Adaptive Moment Estimation)\n    - **ADAGRAD** (Adaptive Gradient Algorithm)\n    - **ADADELTA** (Adaptive Delta)\n    - **RMSPROP** (Root Mean Square Propagation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\nimport numpy as np\n\nfrom DLL.DeepLearning.Optimisers import LBFGS, ADAM, SGD, ADAGRAD, ADADELTA, RMSPROP\n\n\ntorch.set_printoptions(sci_mode=False)\n\ndef f(x):\n    return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n\ndef f_prime(x):\n    return torch.tensor([-2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0] ** 2), 200 * (x[1] - x[0] ** 2)], dtype=x.dtype)\n\ninitial_point = [-0.11, 2.5]\nx1 = torch.tensor(initial_point, dtype=torch.float32)\nx2 = torch.tensor(initial_point, dtype=torch.float32)\nx3 = torch.tensor(initial_point, dtype=torch.float32)\nx4 = torch.tensor(initial_point, dtype=torch.float32)\nx5 = torch.tensor(initial_point, dtype=torch.float32)\nx6 = torch.tensor(initial_point, dtype=torch.float32)\noptimiser1 = LBFGS(lambda: f(x1), history_size=10, maxiterls=20, learning_rate=0.1)\noptimiser2 = SGD(learning_rate=0.001)\noptimiser3 = ADAM(learning_rate=1, amsgrad=True)\noptimiser4 = ADAGRAD(learning_rate=2)\noptimiser5 = ADADELTA(learning_rate=10)\noptimiser6 = RMSPROP(learning_rate=0.1, momentum=0.5)\noptimiser1.initialise_parameters([x1])\noptimiser2.initialise_parameters([x2])\noptimiser3.initialise_parameters([x3])\noptimiser4.initialise_parameters([x4])\noptimiser5.initialise_parameters([x5])\noptimiser6.initialise_parameters([x6])\n\n# with 30 iterations, LBFGS algorithms (my and scipy) converge, while ADAM and SGD converge with around 1000 iterations using initial_point == [-0.11, 2.5]. ADAGRAD takes around 5000.\nmax_iter = 30\nmax_iter = 5000\npoints1 = [x1.clone()]\npoints2 = [x2.clone()]\npoints3 = [x3.clone()]\npoints4 = [x4.clone()]\npoints5 = [x5.clone()]\npoints6 = [x6.clone()]\nfor epoch in range(max_iter):\n    optimiser1.zero_grad()\n    optimiser2.zero_grad()\n    optimiser3.zero_grad()\n    optimiser4.zero_grad()\n    optimiser5.zero_grad()\n    optimiser6.zero_grad()\n    x1.grad += f_prime(x1)\n    x2.grad += f_prime(x2)\n    x3.grad += f_prime(x3)\n    x4.grad += f_prime(x4)\n    x5.grad += f_prime(x5)\n    x6.grad += f_prime(x6)\n    optimiser1.update_parameters()\n    optimiser2.update_parameters()\n    optimiser3.update_parameters()\n    optimiser4.update_parameters()\n    optimiser5.update_parameters()\n    optimiser6.update_parameters()\n    points1.append(x1.clone())\n    points2.append(x2.clone())\n    points3.append(x3.clone())\n    points4.append(x4.clone())\n    points5.append(x5.clone())\n    points6.append(x6.clone())\n    # x = x1\n    # print(f\"Epochs: {epoch + 1}, f(x): {f(x)}, ||f'(x)||_2: {torch.linalg.norm(f_prime(x))}, x: {[round(num, 3) for num in x.tolist()]}\")\n\nscipy_points = []\ndef scipy_func(x):\n    scipy_points.append(x)\n    return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n\nminimize(scipy_func, initial_point, method=\"L-BFGS-B\", options={\"maxiter\": max_iter})  # options={\"maxls\": 1, \"maxiter\": max_iter}\n\n\ncolors = plt.cm.gist_rainbow(np.linspace(0, 1, 6))\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=colors)\n\npoints1 = torch.stack(points1)\npoints2 = torch.stack(points2)\npoints3 = torch.stack(points3)\npoints4 = torch.stack(points4)\npoints5 = torch.stack(points5)\npoints6 = torch.stack(points6)\nscipy_points = np.stack(scipy_points)\nplt.figure(figsize=(10, 8))\nline_style = (0, (1, 3))  # (0, (1, 20))\nmarker_size = 10\nplt.plot(points1[:, 0], points1[:, 1], linestyle=line_style, markersize=marker_size, label=\"path LBSGF\")\nplt.plot(points2[:, 0], points2[:, 1], linestyle=line_style, markersize=marker_size, label=\"path SGD\")\nplt.plot(points3[:, 0], points3[:, 1], linestyle=line_style, markersize=marker_size, label=\"path ADAM\")\nplt.plot(points4[:, 0], points4[:, 1], linestyle=line_style, markersize=marker_size, label=\"path ADAGRAD\")\nplt.plot(points5[:, 0], points5[:, 1], linestyle=line_style, markersize=marker_size, label=\"path ADADELTA\")\nplt.plot(points6[:, 0], points6[:, 1], linestyle=line_style, markersize=marker_size, label=\"path RMSPROP\")\nplt.plot(scipy_points[:, 0], scipy_points[:, 1], linestyle=line_style, markersize=marker_size, label=\"path SCIPY LBSGF\")\nplt.plot(1, 1, \"o\", markersize=15, label=\"optimum\")\nplt.plot(*points1[0], \"o\", markersize=15, label=\"start\")\n\nx_vals = torch.linspace(-2, 2, 100)\ny_vals = torch.linspace(-2.5, 3, 100)\nX, Y = torch.meshgrid(x_vals, y_vals, indexing=\"ij\")\nZ = f([X, Y])\n\ncontour = plt.contourf(X.numpy(), Y.numpy(), Z.numpy(), levels=100, cmap=\"viridis\")\nplt.colorbar(contour)\nplt.legend()\nplt.xlim(-2, 2)\nplt.ylim(-2.5, 3)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}