

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Core layers &mdash; Deep learning library 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=80a70a73" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=8d563738"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Losses" href="DLL.DeepLearning.Losses.html" />
    <link rel="prev" title="Deep learning" href="DLL.DeepLearning.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Deep learning library
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="DLL.html">DLL</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="DLL.Data.html">Data</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="DLL.DeepLearning.html">Deep learning</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Core layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#input">Input</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dense">Dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conv2d">Conv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="#flatten">Flatten</a></li>
<li class="toctree-l4"><a class="reference internal" href="#lstm">LSTM</a></li>
<li class="toctree-l4"><a class="reference internal" href="#maxpooling2d">MaxPooling2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rnn">RNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="#reshape">Reshape</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#activation-layers">Activation layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#relu">ReLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sigmoid">Sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="#softmax">SoftMax</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tanh">Tanh</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#regularisation-layers">Regularisation layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#dropout">Dropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="#batchnormalisation">BatchNormalisation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#groupnormalisation">GroupNormalisation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#instancenormalisation">InstanceNormalisation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#layernormalisation">LayerNormalisation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="DLL.DeepLearning.Losses.html">Losses</a></li>
<li class="toctree-l3"><a class="reference internal" href="DLL.DeepLearning.Optimisers.html">Optimisers</a></li>
<li class="toctree-l3"><a class="reference internal" href="DLL.DeepLearning.Models.html">Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="DLL.MachineLearning.html">MachineLearning</a></li>
<li class="toctree-l2"><a class="reference internal" href="DLL.Exceptions.html">Custom exceptions</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Deep learning library</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="DLL.html">DLL</a></li>
          <li class="breadcrumb-item"><a href="DLL.DeepLearning.html">Deep learning</a></li>
      <li class="breadcrumb-item active">Core layers</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/api/DLL.DeepLearning.Layers.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="core-layers">
<span id="layers-section-label"></span><h1>Core layers<a class="headerlink" href="#core-layers" title="Link to this heading"></a></h1>
<section id="input">
<h2>Input<a class="headerlink" href="#input" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Input">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">Input</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_shape</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Input.html#Input"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Input" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Input.summary">
<span class="sig-name descname"><span class="pre">summary</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Input.html#Input.summary"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Input.summary" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="dense">
<h2>Dense<a class="headerlink" href="#dense" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Dense">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">Dense</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initialiser</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Xavier_uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalisation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Dense.html#Dense"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Dense" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The basic dense linear layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_shape</strong> (<em>int</em>) – The output_shape of the model not containing the batch_size dimension. Must be a non-negative int. If is zero, the returned tensor is of shape (n_samples,) and if positive, the returned tensor is of shape (n_samples, output_shape).</p></li>
<li><p><strong>initialiser</strong> (<em>str</em><em>, </em><em>optional</em>) – The initialisation method for models weights. Xavier should be used for tanh, sigmoid, softmax or other activations, which are approximately linear close to origin, while He should be used for the ReLU activation. Must be one of “Xavier_norm”, “Xavier_uniform”, “He_norm” or “He_uniform”. Defaults to “Xavier_uniform”.</p></li>
<li><p><strong>activation</strong> (<a class="reference internal" href="#activations-section-label"><span class="std std-ref">Activation layers</span></a> | None, optional) – The activation used after this layer. If is set to None, no activation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
<li><p><strong>normalisation</strong> (<a class="reference internal" href="#regularisation-layers-section-label"><span class="std std-ref">Regularisation layers</span></a> | None, optional) – The regularisation layer used after this layer. If is set to None, no regularisation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Dense.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Dense.html#Dense.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Dense.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>,</em><em>) </em><em>if layer.output_shape</em><em>[</em><em>0</em><em>] </em><em>== 0 else</em><em> (</em><em>n_samples</em><em>, </em><em>layer.output_shape</em><em>[</em><em>0</em><em>]</em><em>)</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, layer.input_shape[0])</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Dense.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Dense.html#Dense.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Dense.forward" title="Link to this definition"></a></dt>
<dd><p>Applies the basic linear transformation</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    y_{lin} = xW + b,\\
    y_{reg} = f(y_{lin}),\\
    y_{activ} = g(y_{reg}),
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is the possible regularisation function and <span class="math notranslate nohighlight">\(g\)</span> is the possible activation function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input to the dense layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the transformations with the spesified shape.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples,) if layer.output_shape[0] == 0 else (n_samples, layer.output_shape[0])</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="conv2d">
<h2>Conv2D<a class="headerlink" href="#conv2d" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Conv2D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">Conv2D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_depth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initialiser</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Xavier_uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalisation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Conv2D.html#Conv2D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Conv2D" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The convolutional layer for a neural network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> (<em>int</em>) – The kernel size used for the model. The kernel is automatically square. Must be a positive integer.</p></li>
<li><p><strong>output_depth</strong> (<em>int</em>) – The output depth of the layer. Must be a positive integer.</p></li>
<li><p><strong>initialiser</strong> (<em>str</em><em>, </em><em>optional</em>) – The initialisation method for models weights. Xavier should be used for tanh, sigmoid, softmax or other activations, which are approximately linear close to origin, while He should be used for the ReLU activation. Must be one of “Xavier_norm”, “Xavier_uniform”, “He_norm” or “He_uniform”. Defaults to “Xavier_uniform”.</p></li>
<li><p><strong>activation</strong> (<a class="reference internal" href="#activations-section-label"><span class="std std-ref">Activation layers</span></a> | None, optional) – The activation used after this layer. If is set to None, no activation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
<li><p><strong>normalisation</strong> (<a class="reference internal" href="#regularisation-layers-section-label"><span class="std std-ref">Regularisation layers</span></a> | None, optional) – The regularisation layer used fter this layer. If is set to None, no regularisation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Conv2D.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Conv2D.html#Conv2D.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Conv2D.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>output_depth</em><em>, </em><em>output_height</em><em>, </em><em>output_width</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, input_depth, input_height, input_width)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Conv2D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Conv2D.html#Conv2D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Conv2D.forward" title="Link to this definition"></a></dt>
<dd><p>Applies the convolutional transformation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    y_{i, j} &amp;= \text{bias}_j + \sum_{k = 1}^{\text{d_in}} \text{kernel}(j, k) \star \text{input}(i, k),\\
    y_{reg_{i, j}} &amp;= f(y_{i, j}),\\
    y_{activ_{i, j}} &amp;= g(y_{reg}),
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the cross-correlation operator, <span class="math notranslate nohighlight">\(\text{d_in}\)</span> is the input_depth, <span class="math notranslate nohighlight">\(i\in [1,\dots, \text{batch_size}]\)</span>, <span class="math notranslate nohighlight">\(j\in[1,\dots, \text{output_depth}]\)</span>, <span class="math notranslate nohighlight">\(f\)</span> is the possible regularisation function and <span class="math notranslate nohighlight">\(g\)</span> is the possible activation function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>input_depth</em><em>, </em><em>input_height</em><em>, </em><em>input_width</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the transformations with the spesified shape.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, output_depth, height - kernel_size + 1, width - kernel_size + 1)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="flatten">
<h2>Flatten<a class="headerlink" href="#flatten" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Flatten">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">Flatten</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Flatten.html#Flatten"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Flatten" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The flattening layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Flatten.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Flatten.html#Flatten.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Flatten.backward" title="Link to this definition"></a></dt>
<dd><p>Reshapes the gradient to the original shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>product_of_other_dimensions</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The reshaped gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, <a href="#id1"><span class="problematic" id="id2">*</span></a>layer.input_shape)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Flatten.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Flatten.html#Flatten.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Flatten.forward" title="Link to this definition"></a></dt>
<dd><p>Flattens the input tensor into a 2 dimensional tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after flattening the input tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, product_of_other_dimensions)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="lstm">
<h2>LSTM<a class="headerlink" href="#lstm" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.LSTM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">LSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_last</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initialiser</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Xavier_uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalisation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/LSTM.html#LSTM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.LSTM" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The long short-term memory layer for neural networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_shape</strong> (<em>int</em>) – The number of output features. Must be a non-negative int. If is zero, the returned tensor is of shape (batch_size,) or (batch_size, sequence_length) and if positive, the returned tensor is of shape (batch_size, output_size) or (batch_size, sequence_length, output_size).</p></li>
<li><p><strong>hidden_size</strong> (<em>int</em>) – The number of features in the hidden state vector. Must be a positive integer.</p></li>
<li><p><strong>return_last</strong> (<em>bool</em>) – Determines if only the last element or the whole sequence is returned.</p></li>
<li><p><strong>initialiser</strong> (<em>str</em><em>, </em><em>optional</em>) – The initialisation method for models weights. Xavier should be used for tanh, sigmoid, softmax or other activations, which are approximately linear close to origin, while He should be used for the ReLU activation. Must be one of “Xavier_norm”, “Xavier_uniform”, “He_norm” or “He_uniform”. Defaults to “Xavier_uniform”.</p></li>
<li><p><strong>activation</strong> (<a class="reference internal" href="#activations-section-label"><span class="std std-ref">Activation layers</span></a> | None, optional) – The activation used after this layer. If is set to None, no activation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
<li><p><strong>normalisation</strong> (<a class="reference internal" href="#regularisation-layers-section-label"><span class="std std-ref">Regularisation layers</span></a> | None, optional) – The regularisation layer used after this layer. If is set to None, no regularisation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.LSTM.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/LSTM.html#LSTM.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.LSTM.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, sequence_length, input_size)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.LSTM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/LSTM.html#LSTM.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.LSTM.forward" title="Link to this definition"></a></dt>
<dd><p>Calculates the forward propagation of the model using the equations</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    f_t &amp;= \sigma(W_fx_t + U_fh_{t - 1} + b_f),\\
    i_t &amp;= \sigma(W_ix_t + U_ih_{t - 1} + b_i),\\
    o_t &amp;= \sigma(W_ox_t + U_oh_{t - 1} + b_o),\\
    \widetilde{c}_t &amp;= \text{tanh}(W_cx_t + U_ch_{t - 1} + b_c),\\
    c_t &amp;= f_t\odot c_{t - 1} + i_t\odot\widetilde{c}_t,\\
    h_t &amp;= o_t\odot\text{tanh}(c_t),\\
    y_t &amp;= W_yh_t + b_y,\\
    y_{reg} &amp;= f(y) \text{ or } f(y_\text{sequence_length}),\\
    y_{activ} &amp;= g(y_{reg}),
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(t\in[1,\dots, \text{sequence_length}]\)</span>, <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state, <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(U\)</span> are the weight matricies, <span class="math notranslate nohighlight">\(b\)</span> are the biases, <span class="math notranslate nohighlight">\(f\)</span> is the possible regularisation function and <span class="math notranslate nohighlight">\(g\)</span> is the possible activation function. Also <span class="math notranslate nohighlight">\(\odot\)</span> represents the hadamard or the element-wise product and <span class="math notranslate nohighlight">\(\sigma\)</span> represents the sigmoid function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>sequence_length</em><em>, </em><em>input_size</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>The output tensor after the transformations with the spesified shape.</p>
<table class="docutils align-default" id="id11">
<caption><span class="caption-text">The return shapes of the method depending on the parameters.</span><a class="headerlink" href="#id11" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 28.6%" />
<col style="width: 71.4%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Return Shape</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>LSTM.output_shape[0] == 0 and LSTM.return_last</p></td>
<td><p>(batch_size,)</p></td>
</tr>
<tr class="row-odd"><td><p>LSTM.output_shape[0] &gt; 0 and LSTM.return_last</p></td>
<td><p>(batch_size, output_size)</p></td>
</tr>
<tr class="row-even"><td><p>LSTM.output_shape[0] == 0 and not LSTM.return_last</p></td>
<td><p>(batch_size, sequence_length)</p></td>
</tr>
<tr class="row-odd"><td><p>LSTM.output_shape[0] &gt; 0 and not LSTM.return_last</p></td>
<td><p>(batch_size, sequence_length, output_size)</p></td>
</tr>
</tbody>
</table>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="maxpooling2d">
<h2>MaxPooling2D<a class="headerlink" href="#maxpooling2d" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.MaxPooling2D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">MaxPooling2D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pool_size</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/MaxPooling2D.html#MaxPooling2D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.MaxPooling2D" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Activation</span></code></p>
<p>The max pooling layer for a neural network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>pool_size</strong> (<em>int</em>) – The pooling size used for the model. The pooling kernel is automatically square. Must be a positive integer.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.MaxPooling2D.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/MaxPooling2D.html#MaxPooling2D.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.MaxPooling2D.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>output_depth</em><em>, </em><em>output_height</em><em>, </em><em>output_width</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, input_depth, input_height, input_width)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.MaxPooling2D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/MaxPooling2D.html#MaxPooling2D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.MaxPooling2D.forward" title="Link to this definition"></a></dt>
<dd><p>Applies the max pooling transformation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>input_depth</em><em>, </em><em>height</em><em>, </em><em>width</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the transformations with the spesified shape.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, output_depth, height // layer.pool_size, width // layer.pool_size)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="rnn">
<h2>RNN<a class="headerlink" href="#rnn" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.RNN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">RNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_last</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initialiser</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Xavier_uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalisation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/RNN.html#RNN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.RNN" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The recurrent neural network layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_shape</strong> (<em>int</em>) – The number of output features. Must be a non-negative int. If is zero, the returned tensor is of shape (n_samples,) or (n_samples, sequence_length) and if positive, the returned tensor is of shape (n_samples, output_shape) or (n_samples, sequence_length, output_shape).</p></li>
<li><p><strong>hidden_size</strong> (<em>int</em>) – The number of features in the hidden state vector. Must be a positive integer.</p></li>
<li><p><strong>return_last</strong> (<em>bool</em>) – Determines if only the last element or the whole sequence is returned.</p></li>
<li><p><strong>initialiser</strong> (<em>str</em><em>, </em><em>optional</em>) – The initialisation method for models weights. Xavier should be used for tanh, sigmoid, softmax or other activations, which are approximately linear close to origin, while He should be used for the ReLU activation. Must be one of “Xavier_norm”, “Xavier_uniform”, “He_norm” or “He_uniform”. Defaults to “Xavier_uniform”.</p></li>
<li><p><strong>activation</strong> (<a class="reference internal" href="#activations-section-label"><span class="std std-ref">Activation layers</span></a> | None, optional) – The activation used after this layer. If is set to None, no activation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
<li><p><strong>normalisation</strong> (<a class="reference internal" href="#regularisation-layers-section-label"><span class="std std-ref">Regularisation layers</span></a> | None, optional) – The regularisation layer used after this layer. If is set to None, no regularisation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.RNN.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/RNN.html#RNN.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.RNN.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, sequence_length, input_size)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.RNN.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/RNN.html#RNN.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.RNN.forward" title="Link to this definition"></a></dt>
<dd><p>Calculates the forward propagation of the model using the equation</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    h_t &amp;= \text{tanh}(x_tW_{ih}^T + h_{t - 1}W_{hh}^T + b_h),\\
    y_{t} &amp;= h_tW_o^T + b_o,\\
    y_{reg} &amp;= f(y) \text{ or } f(y_\text{sequence_length}),\\
    y_{activ} &amp;= g(y_{reg}),
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(t\in[1,\dots, \text{sequence_length}]\)</span>, <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state, <span class="math notranslate nohighlight">\(W_{ih}\)</span> is the input to hidden weights, <span class="math notranslate nohighlight">\(W_{hh}\)</span> is the hidden to hidden weights, <span class="math notranslate nohighlight">\(b_h\)</span> is the hidden bias, <span class="math notranslate nohighlight">\(W_o\)</span> is the output weights, <span class="math notranslate nohighlight">\(b_o\)</span> is the output bias, <span class="math notranslate nohighlight">\(f\)</span> is the possible regularisation function and <span class="math notranslate nohighlight">\(g\)</span> is the possible activation function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>sequence_length</em><em>, </em><em>input_size</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>The output tensor after the transformations with the spesified shape.</p>
<table class="docutils align-default" id="id12">
<caption><span class="caption-text">The return shapes of the method depending on the parameters.</span><a class="headerlink" href="#id12" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 28.6%" />
<col style="width: 71.4%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Return Shape</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>RNN.output_shape[0] == 0 and RNN.return_last</p></td>
<td><p>(n_samples,)</p></td>
</tr>
<tr class="row-odd"><td><p>RNN.output_shape[0] &gt; 0 and RNN.return_last</p></td>
<td><p>(n_samples, RNN.output_shape[0])</p></td>
</tr>
<tr class="row-even"><td><p>RNN.output_shape[0] == 0 and not RNN.return_last</p></td>
<td><p>(n_samples, sequence_length)</p></td>
</tr>
<tr class="row-odd"><td><p>RNN.output_shape[0] &gt; 0 and not RNN.return_last</p></td>
<td><p>(n_samples, sequence_length, RNN.output_shape[0])</p></td>
</tr>
</tbody>
</table>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="reshape">
<h2>Reshape<a class="headerlink" href="#reshape" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Reshape">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">Reshape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_shape</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Reshape.html#Reshape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Reshape" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The reshape layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>output_shape</strong> (<em>int</em>) – The output_shape of the model not containing the batch_size dimension. Must be a positive integer or a tuple.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Reshape.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Reshape.html#Reshape.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Reshape.backward" title="Link to this definition"></a></dt>
<dd><p>Reshapes the gradient to the original shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (torch.Tensor of shape (n_samples, <a href="#id3"><span class="problematic" id="id4">*</span></a>layer.output_shape) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The reshaped gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, <a href="#id5"><span class="problematic" id="id6">*</span></a>layer.input_shape)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Reshape.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Reshape.html#Reshape.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Reshape.forward" title="Link to this definition"></a></dt>
<dd><p>Reshapes the input into the output_shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (torch.Tensor of shape (n_samples, <a href="#id7"><span class="problematic" id="id8">*</span></a>layer.input_shape)) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after reshaping the input tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, <a href="#id9"><span class="problematic" id="id10">*</span></a>layer.output_shape)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="activation-layers">
<span id="activations-section-label"></span><h1>Activation layers<a class="headerlink" href="#activation-layers" title="Link to this heading"></a></h1>
<section id="relu">
<h2>ReLU<a class="headerlink" href="#relu" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.ReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Activations.</span></span><span class="sig-name descname"><span class="pre">ReLU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/ReLU.html#ReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.ReLU" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Activation</span></code></p>
<p>The basic rectified linear unit activation function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.ReLU.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/ReLU.html#ReLU.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.ReLU.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.ReLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/ReLU.html#ReLU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.ReLU.forward" title="Link to this definition"></a></dt>
<dd><p>Calculates the following function for every element of the input matrix:</p>
<div class="math notranslate nohighlight">
\[\text{ReLU}(x) = \text{max}(0, x).\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of any shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after applying the activation function of the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="sigmoid">
<h2>Sigmoid<a class="headerlink" href="#sigmoid" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.Sigmoid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Activations.</span></span><span class="sig-name descname"><span class="pre">Sigmoid</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/Sigmoid.html#Sigmoid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.Sigmoid" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Activation</span></code></p>
<p>The sigmoid activation function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.Sigmoid.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/Sigmoid.html#Sigmoid.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.Sigmoid.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.Sigmoid.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/Sigmoid.html#Sigmoid.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.Sigmoid.forward" title="Link to this definition"></a></dt>
<dd><p>Calculates the following function for every element of the input matrix:</p>
<div class="math notranslate nohighlight">
\[\text{\sigma}(x) = \frac{1}{1 + e^{-x}}.\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of any shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after applying the activation function of the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="softmax">
<h2>SoftMax<a class="headerlink" href="#softmax" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.SoftMax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Activations.</span></span><span class="sig-name descname"><span class="pre">SoftMax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/SoftMax.html#SoftMax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.SoftMax" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Activation</span></code></p>
<p>The softmax activation function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.SoftMax.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/SoftMax.html#SoftMax.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.SoftMax.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.SoftMax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/SoftMax.html#SoftMax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.SoftMax.forward" title="Link to this definition"></a></dt>
<dd><p>Calculates the following function for every element of the input matrix:</p>
<div class="math notranslate nohighlight">
\[\text{Softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}},\]</div>
<p>where <span class="math notranslate nohighlight">\(K\)</span> is the number of features of the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after applying the activation function of the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="tanh">
<h2>Tanh<a class="headerlink" href="#tanh" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.Tanh">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Activations.</span></span><span class="sig-name descname"><span class="pre">Tanh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/Tanh.html#Tanh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.Tanh" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Activation</span></code></p>
<p>The hyperbolic tangent activation function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.Tanh.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/Tanh.html#Tanh.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.Tanh.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.Tanh.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/Tanh.html#Tanh.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.Tanh.forward" title="Link to this definition"></a></dt>
<dd><p>Calculates the hyperbolic tangent function for every element of the input matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of any shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after applying the activation function of the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="regularisation-layers">
<span id="regularisation-layers-section-label"></span><h1>Regularisation layers<a class="headerlink" href="#regularisation-layers" title="Link to this heading"></a></h1>
<section id="dropout">
<h2>Dropout<a class="headerlink" href="#dropout" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.Dropout">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Regularisation.</span></span><span class="sig-name descname"><span class="pre">Dropout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/Dropout.html#Dropout"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.Dropout" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseRegularisation</span></code></p>
<p>The dropout layer for neural networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>p</strong> (<em>float</em><em>, </em><em>optional</em>) – The probability of a node being dropped out. Must be strictly between 0 and 1. Defaults to 0.5.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.Dropout.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/Dropout.html#Dropout.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.Dropout.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, channels, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.Dropout.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/Dropout.html#Dropout.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.Dropout.forward" title="Link to this definition"></a></dt>
<dd><p>Sets some values of the input to zero with probability p.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>channels</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the transformation with the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="batchnormalisation">
<h2>BatchNormalisation<a class="headerlink" href="#batchnormalisation" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.BatchNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Regularisation.</span></span><span class="sig-name descname"><span class="pre">BatchNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">patience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/BatchNormalisation.html#BatchNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.BatchNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseRegularisation</span></code></p>
<p>The batch normalisation layer for neural networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>patience</strong> (<em>float</em><em>, </em><em>optional</em>) – The number deciding how fast the mean and variance in training. Must be strictly between 0 and 1. Defaults to 0.9.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.BatchNorm.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/BatchNormalisation.html#BatchNorm.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.BatchNorm.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, channels, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.BatchNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/BatchNormalisation.html#BatchNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.BatchNorm.forward" title="Link to this definition"></a></dt>
<dd><p>Normalises the input to have zero mean and one variance with the following equation:</p>
<div class="math notranslate nohighlight">
\[y = \gamma\frac{x - \mathbb{E}[x]}{\sqrt{\text{var}(x) + \epsilon}} + \beta,\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(\mathbb{E}[x]\)</span> is the expected value or the mean accross the batch dimension, <span class="math notranslate nohighlight">\(\text{var}(x)\)</span> is the variance accross the variance accross the batch dimension, <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are trainable parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>channels</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the normalisation with the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="groupnormalisation">
<h2>GroupNormalisation<a class="headerlink" href="#groupnormalisation" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.GroupNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Regularisation.</span></span><span class="sig-name descname"><span class="pre">GroupNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/GroupNormalisation.html#GroupNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.GroupNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseRegularisation</span></code></p>
<p>The group normalisation layer for neural networks. Computes the group norm of a batch along axis=1</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>num_groups</strong> (<em>int</em><em>, </em><em>optional</em>) – The number of groups used in the normalisation. Must be a positive integer. Defaults to 32. The number of channels must be evenly divisible by num_groups. If is set to 1, is identical to layer normalisation and if batch_size, is identical to the instance normalisation.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.GroupNorm.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/GroupNormalisation.html#GroupNorm.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.GroupNorm.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, channels, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.GroupNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/GroupNormalisation.html#GroupNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.GroupNorm.forward" title="Link to this definition"></a></dt>
<dd><p>Normalises the input to have zero mean and one variance accross self.num_groups groups accross the channel dimension with the following equation:</p>
<div class="math notranslate nohighlight">
\[y = \gamma\frac{x - \mathbb{E}[x]}{\sqrt{\text{var}(x) + \epsilon}} + \beta,\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(\mathbb{E}[x]\)</span> is the expected value or the mean accross each group, <span class="math notranslate nohighlight">\(\text{var}(x)\)</span> is the variance accross the variance accross each group, <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are trainable parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>channels</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the normalisation with the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="instancenormalisation">
<h2>InstanceNormalisation<a class="headerlink" href="#instancenormalisation" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.InstanceNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Regularisation.</span></span><span class="sig-name descname"><span class="pre">InstanceNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/InstanceNormalisation.html#InstanceNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.InstanceNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#DLL.DeepLearning.Layers.Regularisation.GroupNorm" title="DLL.DeepLearning.Layers.Regularisation.GroupNormalisation.GroupNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">GroupNorm</span></code></a></p>
<p>The instance normalisation layer for neural networks. Computes the group norm of a batch along axis=1 with the same number of groups as channels.</p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.InstanceNorm.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/InstanceNormalisation.html#InstanceNorm.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.InstanceNorm.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, channels, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.InstanceNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/InstanceNormalisation.html#InstanceNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.InstanceNorm.forward" title="Link to this definition"></a></dt>
<dd><p>Normalises the input to have zero mean and one variance accross the channel dimension with the following equation:</p>
<div class="math notranslate nohighlight">
\[y = \gamma\frac{x - \mathbb{E}[x]}{\sqrt{\text{var}(x) + \epsilon}} + \beta,\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(\mathbb{E}[x]\)</span> is the expected value or the mean accross the channel dimension, <span class="math notranslate nohighlight">\(\text{var}(x)\)</span> is the variance accross the variance accross the channel dimension, <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are trainable parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>channels</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the normalisation with the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="layernormalisation">
<h2>LayerNormalisation<a class="headerlink" href="#layernormalisation" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.LayerNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Regularisation.</span></span><span class="sig-name descname"><span class="pre">LayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/LayerNormalisation.html#LayerNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.LayerNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#DLL.DeepLearning.Layers.Regularisation.GroupNorm" title="DLL.DeepLearning.Layers.Regularisation.GroupNormalisation.GroupNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">GroupNorm</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.LayerNorm.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/LayerNormalisation.html#LayerNorm.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.LayerNorm.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, channels, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.LayerNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/LayerNormalisation.html#LayerNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.LayerNorm.forward" title="Link to this definition"></a></dt>
<dd><p>Normalises the input to have zero mean and one variance accross the channel dimension with the following equation:</p>
<div class="math notranslate nohighlight">
\[y = \gamma\frac{x - \mathbb{E}[x]}{\sqrt{\text{var}(x) + \epsilon}} + \beta,\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(\mathbb{E}[x]\)</span> is the expected value or the mean accross the channel dimension, <span class="math notranslate nohighlight">\(\text{var}(x)\)</span> is the variance accross the variance accross the channel dimension, <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are trainable parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>channels</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the normalisation with the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="DLL.DeepLearning.html" class="btn btn-neutral float-left" title="Deep learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="DLL.DeepLearning.Losses.html" class="btn btn-neutral float-right" title="Losses" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Aatu Selkee.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>