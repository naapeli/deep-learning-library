

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Core layers &mdash; Deep learning library 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=e2bd21bd" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=8d563738"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Losses" href="DLL.DeepLearning.Losses.html" />
    <link rel="prev" title="Deep learning" href="DLL.DeepLearning.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Deep learning library
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="DLL.html">DLL</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="DLL.Data.html">Data</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="DLL.DeepLearning.html">Deep learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="DLL.MachineLearning.html">MachineLearning</a></li>
<li class="toctree-l2"><a class="reference internal" href="DLL.Exceptions.html">Custom exceptions</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../auto_examples/first.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#deep-learning">Deep learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#gaussian-processes">Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#linear-models">Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#metrics">Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#naive-bayes">Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#neighbours">Neighbours</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#optimizers">Optimizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#reinforcement-learning">Reinforcement learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#support-vector-machines">Support vector machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#trees-and-boosting-machines">Trees and boosting machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/index.html#unsupervised-learning">Unsupervised learning</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Deep learning library</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="DLL.html">DLL</a></li>
          <li class="breadcrumb-item"><a href="DLL.DeepLearning.html">Deep learning</a></li>
      <li class="breadcrumb-item active">Core layers</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/api/DLL.DeepLearning.Layers.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="core-layers">
<span id="layers-section-label"></span><h1>Core layers<a class="headerlink" href="#core-layers" title="Link to this heading"></a></h1>
<section id="dense">
<h2>Dense<a class="headerlink" href="#dense" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Dense">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">Dense</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initialiser=&lt;DLL.DeepLearning.Initialisers._Xavier_Glorot.Xavier_Uniform</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalisation=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Dense.html#Dense"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Dense" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The basic dense linear layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_shape</strong> (<em>tuple</em><em>[</em><em>int</em><em>] or </em><em>int</em>) – The output_shape of the model not containing the batch_size dimension. Must contain non-negative integers. If is int, returned shape is (n_samples, int). If is the length is zero, the returned tensor is of shape (n_samples,). Otherwise the returned tensor is of shape (n_samples, <a href="#id1"><span class="problematic" id="id2">*</span></a>output_shape).</p></li>
<li><p><strong>initialiser</strong> (<a class="reference internal" href="DLL.DeepLearning.Initialisers.html#initialisers-section-label"><span class="std std-ref">Initialisers</span></a>, optional) – The initialisation method for models weights. Defaults to Xavier_uniform.</p></li>
<li><p><strong>activation</strong> (<a class="reference internal" href="#activations-section-label"><span class="std std-ref">Activation layers</span></a> | None, optional) – The activation used after this layer. If is set to None, no activation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
<li><p><strong>normalisation</strong> (<a class="reference internal" href="#regularisation-layers-section-label"><span class="std std-ref">Regularisation layers</span></a> | None, optional) – The regularisation layer used after this layer. If is set to None, no regularisation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Dense.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Dense.html#Dense.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Dense.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>,</em><em>) </em><em>if len</em><em>(</em><em>layer.output_shape</em><em>) </em><em>== 0 else</em><em> (</em><em>n_samples</em><em>, </em><em>output_shape</em><em>)</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, layer.input_shape[0])</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Dense.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Dense.html#Dense.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Dense.forward" title="Link to this definition"></a></dt>
<dd><p>Applies the basic linear transformation</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    y_{lin} = xW + b,\\
    y_{reg} = f(y_{lin}),\\
    y_{activ} = g(y_{reg}),
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is the possible regularisation function and <span class="math notranslate nohighlight">\(g\)</span> is the possible activation function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input to the dense layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the transformations with the spesified shape.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples,) if len(layer.output_shape) == 0 else (n_samples, output_shape)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="conv2d">
<h2>Conv2D<a class="headerlink" href="#conv2d" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Conv2D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">Conv2D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_depth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initialiser=&lt;DLL.DeepLearning.Initialisers._Xavier_Glorot.Xavier_Uniform</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalisation=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Conv2D.html#Conv2D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Conv2D" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The convolutional layer for a neural network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> (<em>int</em>) – The kernel size used for the model. The kernel is automatically square. Must be a positive integer.</p></li>
<li><p><strong>output_depth</strong> (<em>int</em>) – The output depth of the layer. Must be a positive integer.</p></li>
<li><p><strong>initialiser</strong> (<a class="reference internal" href="DLL.DeepLearning.Initialisers.html#initialisers-section-label"><span class="std std-ref">Initialisers</span></a>, optional) – The initialisation method for models weights. Defaults to Xavier_uniform.</p></li>
<li><p><strong>activation</strong> (<a class="reference internal" href="#activations-section-label"><span class="std std-ref">Activation layers</span></a> | None, optional) – The activation used after this layer. If is set to None, no activation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
<li><p><strong>normalisation</strong> (<a class="reference internal" href="#regularisation-layers-section-label"><span class="std std-ref">Regularisation layers</span></a> | None, optional) – The regularisation layer used fter this layer. If is set to None, no regularisation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Conv2D.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Conv2D.html#Conv2D.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Conv2D.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>output_depth</em><em>, </em><em>output_height</em><em>, </em><em>output_width</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, input_depth, input_height, input_width)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Conv2D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Conv2D.html#Conv2D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Conv2D.forward" title="Link to this definition"></a></dt>
<dd><p>Applies the convolutional transformation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    y_{i, j} &amp;= \text{bias}_j + \sum_{k = 1}^{\text{d_in}} \text{kernel}(j, k) \star \text{input}(i, k),\\
    y_{reg_{i, j}} &amp;= f(y_{i, j}),\\
    y_{activ_{i, j}} &amp;= g(y_{reg}),
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the cross-correlation operator, <span class="math notranslate nohighlight">\(\text{d_in}\)</span> is the input_depth, <span class="math notranslate nohighlight">\(i\in [1,\dots, \text{batch_size}]\)</span>, <span class="math notranslate nohighlight">\(j\in[1,\dots, \text{output_depth}]\)</span>, <span class="math notranslate nohighlight">\(f\)</span> is the possible regularisation function and <span class="math notranslate nohighlight">\(g\)</span> is the possible activation function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>input_depth</em><em>, </em><em>input_height</em><em>, </em><em>input_width</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the transformations with the spesified shape.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, output_depth, height - kernel_size + 1, width - kernel_size + 1)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="flatten">
<h2>Flatten<a class="headerlink" href="#flatten" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Flatten">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">Flatten</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Flatten.html#Flatten"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Flatten" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The flattening layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Flatten.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Flatten.html#Flatten.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Flatten.backward" title="Link to this definition"></a></dt>
<dd><p>Reshapes the gradient to the original shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>product_of_other_dimensions</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The reshaped gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, <a href="#id3"><span class="problematic" id="id4">*</span></a>layer.input_shape)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Flatten.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Flatten.html#Flatten.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Flatten.forward" title="Link to this definition"></a></dt>
<dd><p>Flattens the input tensor into a 2 dimensional tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after flattening the input tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, product_of_other_dimensions)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="lstm">
<h2>LSTM<a class="headerlink" href="#lstm" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.LSTM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">LSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_last=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initialiser=&lt;DLL.DeepLearning.Initialisers._Xavier_Glorot.Xavier_Uniform</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalisation=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_LSTM.html#LSTM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.LSTM" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The long short-term memory layer for neural networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_shape</strong> (<em>tuple</em><em>[</em><em>int</em><em>]</em>) – The ouput shape by the forward method. Must be tuple containing non-negative ints. Based on the length of the tuple and the return_last parameter, the returned tensor is of shape (n_samples,), (n_samples, sequence_length), (n_samples, n_features) or (n_samples, sequence_length, n_features).</p></li>
<li><p><strong>hidden_size</strong> (<em>int</em>) – The number of features in the hidden state vector. Must be a positive integer.</p></li>
<li><p><strong>return_last</strong> (<em>bool</em>) – Determines if only the last element or the whole sequence is returned.</p></li>
<li><p><strong>initialiser</strong> (<a class="reference internal" href="DLL.DeepLearning.Initialisers.html#initialisers-section-label"><span class="std std-ref">Initialisers</span></a>, optional) – The initialisation method for models weights. Defaults to Xavier_uniform.</p></li>
<li><p><strong>activation</strong> (<a class="reference internal" href="#activations-section-label"><span class="std std-ref">Activation layers</span></a> | None, optional) – The activation used after this layer. If is set to None, no activation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
<li><p><strong>normalisation</strong> (<a class="reference internal" href="#regularisation-layers-section-label"><span class="std std-ref">Regularisation layers</span></a> | None, optional) – The regularisation layer used after this layer. If is set to None, no regularisation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.LSTM.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_LSTM.html#LSTM.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.LSTM.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, sequence_length, input_size)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.LSTM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_LSTM.html#LSTM.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.LSTM.forward" title="Link to this definition"></a></dt>
<dd><p>Calculates the forward propagation of the model using the equations</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    f_t &amp;= \sigma(W_fx_t + U_fh_{t - 1} + b_f),\\
    i_t &amp;= \sigma(W_ix_t + U_ih_{t - 1} + b_i),\\
    o_t &amp;= \sigma(W_ox_t + U_oh_{t - 1} + b_o),\\
    \widetilde{c}_t &amp;= \text{tanh}(W_cx_t + U_ch_{t - 1} + b_c),\\
    c_t &amp;= f_t\odot c_{t - 1} + i_t\odot\widetilde{c}_t,\\
    h_t &amp;= o_t\odot\text{tanh}(c_t),\\
    y_t &amp;= W_yh_t + b_y,\\
    y_{reg} &amp;= f(y) \text{ or } f(y_\text{sequence_length}),\\
    y_{activ} &amp;= g(y_{reg}),
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(t\in[1,\dots, \text{sequence_length}]\)</span>, <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state, <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(U\)</span> are the weight matricies, <span class="math notranslate nohighlight">\(b\)</span> are the biases, <span class="math notranslate nohighlight">\(f\)</span> is the possible regularisation function and <span class="math notranslate nohighlight">\(g\)</span> is the possible activation function. Also <span class="math notranslate nohighlight">\(\odot\)</span> represents the hadamard or the element-wise product and <span class="math notranslate nohighlight">\(\sigma\)</span> represents the sigmoid function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>sequence_length</em><em>, </em><em>input_size</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>The output tensor after the transformations with the spesified shape.</p>
<table class="docutils align-default" id="id15">
<caption><span class="caption-text">The return shapes of the method depending on the parameters.</span><a class="headerlink" href="#id15" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 28.6%" />
<col style="width: 71.4%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Return Shape</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>len(LSTM.output_shape) == 0 and LSTM.return_last</p></td>
<td><p>(n_samples,)</p></td>
</tr>
<tr class="row-odd"><td><p>len(LSTM.output_shape) == 1 and LSTM.return_last</p></td>
<td><p>(n_samples, LSTM.output_shape[1])</p></td>
</tr>
<tr class="row-even"><td><p>len(LSTM.output_shape) == 1 and not LSTM.return_last</p></td>
<td><p>(n_samples, sequence_length)</p></td>
</tr>
<tr class="row-odd"><td><p>len(LSTM.output_shape) == 2 and not LSTM.return_last</p></td>
<td><p>(n_samples, sequence_length, LSTM.output_shape[1])</p></td>
</tr>
</tbody>
</table>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="maxpooling2d">
<h2>MaxPooling2D<a class="headerlink" href="#maxpooling2d" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.MaxPooling2D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">MaxPooling2D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pool_size</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_MaxPooling2D.html#MaxPooling2D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.MaxPooling2D" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Activation</span></code></p>
<p>The max pooling layer for a neural network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>pool_size</strong> (<em>int</em>) – The pooling size used for the model. The pooling kernel is automatically square. Must be a positive integer.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.MaxPooling2D.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_MaxPooling2D.html#MaxPooling2D.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.MaxPooling2D.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>output_depth</em><em>, </em><em>output_height</em><em>, </em><em>output_width</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, input_depth, input_height, input_width)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.MaxPooling2D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_MaxPooling2D.html#MaxPooling2D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.MaxPooling2D.forward" title="Link to this definition"></a></dt>
<dd><p>Applies the max pooling transformation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>input_depth</em><em>, </em><em>height</em><em>, </em><em>width</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the transformations with the spesified shape.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, output_depth, height // layer.pool_size, width // layer.pool_size)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="rnn">
<h2>RNN<a class="headerlink" href="#rnn" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.RNN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">RNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_last=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initialiser=&lt;DLL.DeepLearning.Initialisers._Xavier_Glorot.Xavier_Uniform</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalisation=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_RNN.html#RNN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.RNN" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The recurrent neural network layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_shape</strong> (<em>tuple</em><em>[</em><em>int</em><em>]</em>) – The ouput shape by the forward method. Must be tuple containing non-negative ints. Based on the length of the tuple and the return_last parameter, the returned tensor is of shape (n_samples,), (n_samples, sequence_length), (n_samples, n_features) or (n_samples, sequence_length, n_features).</p></li>
<li><p><strong>hidden_size</strong> (<em>int</em>) – The number of features in the hidden state vector. Must be a positive integer.</p></li>
<li><p><strong>return_last</strong> (<em>bool</em>) – Determines if only the last element or the whole sequence is returned.</p></li>
<li><p><strong>initialiser</strong> (<a class="reference internal" href="DLL.DeepLearning.Initialisers.html#initialisers-section-label"><span class="std std-ref">Initialisers</span></a>, optional) – The initialisation method for models weights. Defaults to Xavier_uniform.</p></li>
<li><p><strong>activation</strong> (<a class="reference internal" href="#activations-section-label"><span class="std std-ref">Activation layers</span></a> | None, optional) – The activation used after this layer. If is set to None, no activation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
<li><p><strong>normalisation</strong> (<a class="reference internal" href="#regularisation-layers-section-label"><span class="std std-ref">Regularisation layers</span></a> | None, optional) – The regularisation layer used after this layer. If is set to None, no regularisation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.RNN.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_RNN.html#RNN.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.RNN.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, sequence_length, input_size)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.RNN.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_RNN.html#RNN.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.RNN.forward" title="Link to this definition"></a></dt>
<dd><p>Calculates the forward propagation of the model using the equation</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    h_t &amp;= \text{tanh}(x_tW_{ih}^T + h_{t - 1}W_{hh}^T + b_h),\\
    y_{t} &amp;= h_tW_o^T + b_o,\\
    y_{reg} &amp;= f(y) \text{ or } f(y_\text{sequence_length}),\\
    y_{activ} &amp;= g(y_{reg}),
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(t\in[1,\dots, \text{sequence_length}]\)</span>, <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state, <span class="math notranslate nohighlight">\(W_{ih}\)</span> is the input to hidden weights, <span class="math notranslate nohighlight">\(W_{hh}\)</span> is the hidden to hidden weights, <span class="math notranslate nohighlight">\(b_h\)</span> is the hidden bias, <span class="math notranslate nohighlight">\(W_o\)</span> is the output weights, <span class="math notranslate nohighlight">\(b_o\)</span> is the output bias, <span class="math notranslate nohighlight">\(f\)</span> is the possible regularisation function and <span class="math notranslate nohighlight">\(g\)</span> is the possible activation function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>sequence_length</em><em>, </em><em>input_size</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>The output tensor after the transformations with the spesified shape.</p>
<table class="docutils align-default" id="id16">
<caption><span class="caption-text">The return shapes of the method depending on the parameters.</span><a class="headerlink" href="#id16" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 28.6%" />
<col style="width: 71.4%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Return Shape</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>len(RNN.output_shape) == 0 and RNN.return_last</p></td>
<td><p>(n_samples,)</p></td>
</tr>
<tr class="row-odd"><td><p>len(RNN.output_shape) == 1 and RNN.return_last</p></td>
<td><p>(n_samples, RNN.output_shape[1])</p></td>
</tr>
<tr class="row-even"><td><p>len(RNN.output_shape) == 1 and not RNN.return_last</p></td>
<td><p>(n_samples, sequence_length)</p></td>
</tr>
<tr class="row-odd"><td><p>len(RNN.output_shape) == 2 and not RNN.return_last</p></td>
<td><p>(n_samples, sequence_length, RNN.output_shape[1])</p></td>
</tr>
</tbody>
</table>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="bidirectional">
<h2>Bidirectional<a class="headerlink" href="#bidirectional" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Bidirectional">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">Bidirectional</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Bidirectional.html#Bidirectional"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Bidirectional" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The bidirectional wrapper for LSTM or RNN layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>layer</strong> (<a class="reference internal" href="#DLL.DeepLearning.Layers.RNN" title="DLL.DeepLearning.Layers.RNN"><em>DLL.DeepLearning.Layers.RNN</em></a><em> or </em><em>LSTM object</em>) – The input is passed to this layer in forward and reverse. The results of each layer are concatanated together along the feature axis.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Bidirectional.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Bidirectional.html#Bidirectional.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Bidirectional.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, sequence_length, input_size)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Bidirectional.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Bidirectional.html#Bidirectional.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Bidirectional.forward" title="Link to this definition"></a></dt>
<dd><p>Computes the forward values of the RNN or LSTM layer for both normal input and reverse input and concatanates the results along the feature axis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>sequence_length</em><em>, </em><em>input_size</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the transformations with the spesified shape.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, 2 * RNN.output_shape[-1]) or (n_samples, sequence_length, 2 * RNN.output_shape[-1])</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Bidirectional.get_nparams">
<span class="sig-name descname"><span class="pre">get_nparams</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Bidirectional.html#Bidirectional.get_nparams"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Bidirectional.get_nparams" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Bidirectional.summary">
<span class="sig-name descname"><span class="pre">summary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Bidirectional.html#Bidirectional.summary"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Bidirectional.summary" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="multiheadattention">
<h2>MultiHeadAttention<a class="headerlink" href="#multiheadattention" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.MultiHeadAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">MultiHeadAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalisation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Attention.html#MultiHeadAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.MultiHeadAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The multi head attention layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_shape</strong> (<em>tuple</em><em>[</em><em>int</em><em>]</em>) – The output_shape of the model not containing the batch_size. Must be a tuple of positive integers. The returned tensor is of shape (n_samples, seq_len, output_shape) if len(output_shape) == 2 else (n_samples, seq_len).</p></li>
<li><p><strong>n_heads</strong> (<em>int</em>) – The number of heads used in the layer. The output dimension must be divisible by n_heads.</p></li>
<li><p><strong>use_mask</strong> (<em>bool</em>) – Determines if a mask is used to make the model only consider past tokens. Must be a boolean.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – The probability of a node being dropped out. Must be in range [0, 1). Defaults to 0.0.</p></li>
<li><p><strong>activation</strong> (<a class="reference internal" href="#activations-section-label"><span class="std std-ref">Activation layers</span></a> | None, optional) – The activation used after this layer. If is set to None, no activation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
<li><p><strong>normalisation</strong> (<a class="reference internal" href="#regularisation-layers-section-label"><span class="std std-ref">Regularisation layers</span></a> | None, optional) – The regularisation layer used after this layer. If is set to None, no regularisation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.MultiHeadAttention.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Attention.html#MultiHeadAttention.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.MultiHeadAttention.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>seq_len</em><em>, </em><em>output_dim</em><em>) </em><em>if len</em><em>(</em><em>output_shape</em><em>[</em><em>0</em><em>]</em><em>) </em><em>!= 0 else</em><em> (</em><em>n_samples</em><em>, </em><em>seq_len</em><em>)</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, seq_len, output_dim)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.MultiHeadAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Attention.html#MultiHeadAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.MultiHeadAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Applies the attention mechanism on multiple heads.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    y_{\text{MultiHead}} &amp;= \text{Concat}(head_1, \dots, head_{\text{n_heads}}),\\
    y_{reg} &amp;= f(y_{\text{MultiHead}}),\\
    y_{activ} &amp;= g(y_{reg}),
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(head_i = \text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{\text{output_dim}}})\)</span>, <span class="math notranslate nohighlight">\(f\)</span> is the possible regularisation function and <span class="math notranslate nohighlight">\(g\)</span> is the possible activation function. <span class="math notranslate nohighlight">\(Q, K\)</span> and <span class="math notranslate nohighlight">\(V\)</span> are the query, key and value matricies, which taken from the input by transforming it by a linear layer and splitting the result on the feature axis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>seq_len</em><em>, </em><em>output_shape</em><em>)</em>) – The input to the dense layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the transformations with the spesified shape.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, seq_len) if output_shape == 0 else (n_samples, seq_len, output_shape)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="identity">
<h2>Identity<a class="headerlink" href="#identity" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Identity">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">Identity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Identity.html#Identity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Identity" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Activation</span></code></p>
<p>The identity layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Identity.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Identity.html#Identity.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Identity.backward" title="Link to this definition"></a></dt>
<dd><p>Returns the gradient.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The same tensor as the input gradient</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Identity.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Identity.html#Identity.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Identity.forward" title="Link to this definition"></a></dt>
<dd><p>Returns the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input to the dense layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The same tensor as the input</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="add">
<h2>Add<a class="headerlink" href="#add" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Add">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">Add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalisation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Add.html#Add"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Add" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The addition layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer1</strong> (<em>DLL.DeepLearning.Layers.BaseLayer object</em>) – The first layer the input is passed to. The results of each layer are added together. The input and outpput shapes of the layers must be the same.</p></li>
<li><p><strong>layer2</strong> (<em>DLL.DeepLearning.Layers.BaseLayer object</em>) – The second layer the input is passed to. The results of each layer are added together. The input and outpput shapes of the layers must be the same.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Add.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Add.html#Add.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Add.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layers. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of the spesified shape</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Add.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Add.html#Add.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Add.forward" title="Link to this definition"></a></dt>
<dd><p>Computes the forward values of the input layers and adds them together.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the transformations with the spesified shape.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Add.get_nparams">
<span class="sig-name descname"><span class="pre">get_nparams</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Add.html#Add.get_nparams"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Add.get_nparams" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Add.summary">
<span class="sig-name descname"><span class="pre">summary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Add.html#Add.summary"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Add.summary" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="layerlist">
<h2>LayerList<a class="headerlink" href="#layerlist" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.LayerList">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">LayerList</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_LayerList.html#LayerList"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.LayerList" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The list of consecutive layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>*args</strong> (<em>DLL.DeepLearning.Layers.BaseLayer objects</em>) – An arbitrary amount of consecutive layers.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.LayerList.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_LayerList.html#LayerList.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.LayerList.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layers. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of the spesified shape</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.LayerList.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_LayerList.html#LayerList.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.LayerList.forward" title="Link to this definition"></a></dt>
<dd><p>Computes the forward values of the input layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the layers with the spesified shape.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.LayerList.get_nparams">
<span class="sig-name descname"><span class="pre">get_nparams</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_LayerList.html#LayerList.get_nparams"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.LayerList.get_nparams" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.LayerList.summary">
<span class="sig-name descname"><span class="pre">summary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_LayerList.html#LayerList.summary"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.LayerList.summary" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="reshape">
<h2>Reshape<a class="headerlink" href="#reshape" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Reshape">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">Reshape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_shape</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Reshape.html#Reshape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Reshape" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The reshape layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>output_shape</strong> (<em>int</em>) – The output_shape of the model not containing the batch_size dimension. Must be a positive integer or a tuple.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Reshape.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Reshape.html#Reshape.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Reshape.backward" title="Link to this definition"></a></dt>
<dd><p>Reshapes the gradient to the original shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (torch.Tensor of shape (n_samples, <a href="#id5"><span class="problematic" id="id6">*</span></a>layer.output_shape) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The reshaped gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, <a href="#id7"><span class="problematic" id="id8">*</span></a>layer.input_shape)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Reshape.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_Reshape.html#Reshape.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Reshape.forward" title="Link to this definition"></a></dt>
<dd><p>Reshapes the input into the output_shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (torch.Tensor of shape (n_samples, <a href="#id9"><span class="problematic" id="id10">*</span></a>layer.input_shape)) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after reshaping the input tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, <a href="#id11"><span class="problematic" id="id12">*</span></a>layer.output_shape)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="densekan">
<h2>DenseKAN<a class="headerlink" href="#densekan" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.DenseKAN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.</span></span><span class="sig-name descname"><span class="pre">DenseKAN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_basis_funcs=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounds=(-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">1)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">basis_func_degree=3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initialiser=&lt;DLL.DeepLearning.Initialisers._Xavier_Glorot.Xavier_Uniform</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalisation=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_DenseKAN.html#DenseKAN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.DenseKAN" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseLayer</span></code></p>
<p>The dense Kolmogorov-Arnold network layer. The implementation is based on <a class="reference external" href="https://arxiv.org/pdf/2404.19756">this paper</a> and <a class="reference external" href="https://mlwithouttears.com/2024/05/15/a-from-scratch-implementation-of-kolmogorov-arnold-networks-kan/">this article</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_shape</strong> (<em>tuple</em><em>[</em><em>int</em><em>] or </em><em>int</em>) – The output_shape of the model not containing the batch_size dimension. Must contain non-negative integers. If is int, returned shape is (n_samples, int). If is the length is zero, the returned tensor is of shape (n_samples,). Otherwise the returned tensor is of shape (n_samples, <a href="#id13"><span class="problematic" id="id14">*</span></a>output_shape).</p></li>
<li><p><strong>n_basis_funcs</strong> (<em>int</em><em>, </em><em>optional</em>) – The number of basis functions used for fitting. If 1, only SiLU is used. Otherwise 1 SiLU and n_basis_funcs - 1 Bsplines basis functions are used. Must be a positive integer. Defaults to 10.</p></li>
<li><p><strong>bounds</strong> (<em>tuple</em><em>[</em><em>int</em><em>]</em><em>, </em><em>optional</em>) – The theoretical min and max of the data that will be passed to the forward method. Must be a tuple containing two integers. Defaults to (-1, 1).</p></li>
<li><p><strong>basis_func_degree</strong> (<em>int</em><em>, </em><em>optional</em>) – The degree of the Bspline basis functions. Must be positive. Defaults to 3.</p></li>
<li><p><strong>initialiser</strong> (<a class="reference internal" href="DLL.DeepLearning.Initialisers.html#initialisers-section-label"><span class="std std-ref">Initialisers</span></a>, optional) – The initialisation method for models weights. Defaults to Xavier_uniform.</p></li>
<li><p><strong>activation</strong> (<a class="reference internal" href="#activations-section-label"><span class="std std-ref">Activation layers</span></a> | None, optional) – The activation used after this layer. If is set to None, no activation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
<li><p><strong>normalisation</strong> (<a class="reference internal" href="#regularisation-layers-section-label"><span class="std std-ref">Regularisation layers</span></a> | None, optional) – The regularisation layer used after this layer. If is set to None, no regularisation is used. Defaults to None. If both activation and regularisation is used, the regularisation is performed first in the forward propagation.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>n_basis_funcs and basis_func_degree should be different to avoid certain errors.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.DenseKAN.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_DenseKAN.html#DenseKAN.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.DenseKAN.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>,</em><em>) </em><em>if len</em><em>(</em><em>layer.output_shape</em><em>) </em><em>== 0 else</em><em> (</em><em>n_samples</em><em>, </em><em>output_shape</em><em>)</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, layer.input_shape[0])</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.DenseKAN.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/_DenseKAN.html#DenseKAN.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.DenseKAN.forward" title="Link to this definition"></a></dt>
<dd><p>Applies the forward equation of the Kolmogorov-Arnold network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input to the dense layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the transformations with the spesified shape.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples,) if len(layer.output_shape) == 0 else (n_samples, output_shape)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="activation-layers">
<span id="activations-section-label"></span><h1>Activation layers<a class="headerlink" href="#activation-layers" title="Link to this heading"></a></h1>
<section id="relu">
<h2>ReLU<a class="headerlink" href="#relu" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.ReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Activations.</span></span><span class="sig-name descname"><span class="pre">ReLU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/_ReLU.html#ReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.ReLU" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Activation</span></code></p>
<p>The basic rectified linear unit activation function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.ReLU.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/_ReLU.html#ReLU.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.ReLU.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.ReLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/_ReLU.html#ReLU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.ReLU.forward" title="Link to this definition"></a></dt>
<dd><p>Calculates the following function for every element of the input matrix:</p>
<div class="math notranslate nohighlight">
\[\text{ReLU}(x) = \text{max}(0, x).\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of any shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after applying the activation function of the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="sigmoid">
<h2>Sigmoid<a class="headerlink" href="#sigmoid" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.Sigmoid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Activations.</span></span><span class="sig-name descname"><span class="pre">Sigmoid</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/_Sigmoid.html#Sigmoid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.Sigmoid" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Activation</span></code></p>
<p>The sigmoid activation function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.Sigmoid.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/_Sigmoid.html#Sigmoid.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.Sigmoid.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.Sigmoid.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/_Sigmoid.html#Sigmoid.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.Sigmoid.forward" title="Link to this definition"></a></dt>
<dd><p>Calculates the following function for every element of the input matrix:</p>
<div class="math notranslate nohighlight">
\[\sigma(x) = \frac{1}{1 + e^{-x}}.\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of any shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after applying the activation function of the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="softmax">
<h2>SoftMax<a class="headerlink" href="#softmax" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.SoftMax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Activations.</span></span><span class="sig-name descname"><span class="pre">SoftMax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/_SoftMax.html#SoftMax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.SoftMax" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Activation</span></code></p>
<p>The softmax activation function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dim</strong> (<em>int</em>) – The dimension on which the softmax is calculated. If the data is (n_samples, n_channels, n_features) and one wants to calculate the softmax on the channels, one should select dim=1 or dim=-2.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.SoftMax.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/_SoftMax.html#SoftMax.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.SoftMax.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.SoftMax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/_SoftMax.html#SoftMax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.SoftMax.forward" title="Link to this definition"></a></dt>
<dd><p>Calculates the following function for every element of the input matrix:</p>
<div class="math notranslate nohighlight">
\[\text{Softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}},\]</div>
<p>where <span class="math notranslate nohighlight">\(K\)</span> is the number of features of the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after applying the activation function of the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="tanh">
<h2>Tanh<a class="headerlink" href="#tanh" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.Tanh">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Activations.</span></span><span class="sig-name descname"><span class="pre">Tanh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/_Tanh.html#Tanh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.Tanh" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Activation</span></code></p>
<p>The hyperbolic tangent activation function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.Tanh.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/_Tanh.html#Tanh.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.Tanh.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Activations.Tanh.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Activations/_Tanh.html#Tanh.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Activations.Tanh.forward" title="Link to this definition"></a></dt>
<dd><p>Calculates the hyperbolic tangent function for every element of the input matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of any shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after applying the activation function of the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="regularisation-layers">
<span id="regularisation-layers-section-label"></span><h1>Regularisation layers<a class="headerlink" href="#regularisation-layers" title="Link to this heading"></a></h1>
<section id="dropout">
<h2>Dropout<a class="headerlink" href="#dropout" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.Dropout">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Regularisation.</span></span><span class="sig-name descname"><span class="pre">Dropout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_Dropout.html#Dropout"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.Dropout" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseRegularisation</span></code></p>
<p>The dropout layer for neural networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>p</strong> (<em>float</em><em>, </em><em>optional</em>) – The probability of a node being dropped out. Must be strictly between 0 and 1. Defaults to 0.5.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.Dropout.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_Dropout.html#Dropout.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.Dropout.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, channels, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.Dropout.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_Dropout.html#Dropout.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.Dropout.forward" title="Link to this definition"></a></dt>
<dd><p>Sets some values of the input to zero with probability p.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>channels</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the transformation with the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="batchnormalisation">
<h2>BatchNormalisation<a class="headerlink" href="#batchnormalisation" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.BatchNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Regularisation.</span></span><span class="sig-name descname"><span class="pre">BatchNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">patience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_BatchNormalisation.html#BatchNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.BatchNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseRegularisation</span></code></p>
<p>The batch normalisation layer for neural networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>patience</strong> (<em>float</em><em>, </em><em>optional</em>) – The number deciding how fast the mean and variance in training. Must be strictly between 0 and 1. Defaults to 0.9.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.BatchNorm.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_BatchNormalisation.html#BatchNorm.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.BatchNorm.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, channels, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.BatchNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_BatchNormalisation.html#BatchNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.BatchNorm.forward" title="Link to this definition"></a></dt>
<dd><p>Normalises the input to have zero mean and one variance with the following equation:</p>
<div class="math notranslate nohighlight">
\[y = \gamma\frac{x - \mathbb{E}[x]}{\sqrt{\text{var}(x) + \epsilon}} + \beta,\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(\mathbb{E}[x]\)</span> is the expected value or the mean accross the batch dimension, <span class="math notranslate nohighlight">\(\text{var}(x)\)</span> is the variance accross the variance accross the batch dimension, <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are trainable parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>channels</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the normalisation with the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="groupnormalisation">
<h2>GroupNormalisation<a class="headerlink" href="#groupnormalisation" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.GroupNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Regularisation.</span></span><span class="sig-name descname"><span class="pre">GroupNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_GroupNormalisation.html#GroupNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.GroupNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseRegularisation</span></code></p>
<p>The group normalisation layer for neural networks. Computes the group norm of a batch along axis=1</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>num_groups</strong> (<em>int</em><em>, </em><em>optional</em>) – The number of groups used in the normalisation. Must be a positive integer. Defaults to 32. The number of channels must be evenly divisible by num_groups. If is set to 1, is identical to layer normalisation and if batch_size, is identical to the instance normalisation.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.GroupNorm.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_GroupNormalisation.html#GroupNorm.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.GroupNorm.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, channels, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.GroupNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_GroupNormalisation.html#GroupNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.GroupNorm.forward" title="Link to this definition"></a></dt>
<dd><p>Normalises the input to have zero mean and one variance accross self.num_groups groups accross the channel dimension with the following equation:</p>
<div class="math notranslate nohighlight">
\[y = \gamma\frac{x - \mathbb{E}[x]}{\sqrt{\text{var}(x) + \epsilon}} + \beta,\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(\mathbb{E}[x]\)</span> is the expected value or the mean accross each group, <span class="math notranslate nohighlight">\(\text{var}(x)\)</span> is the variance accross the variance accross each group, <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are trainable parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>channels</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – The boolean flag deciding if the model is in training mode. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the normalisation with the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="instancenormalisation">
<h2>InstanceNormalisation<a class="headerlink" href="#instancenormalisation" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.InstanceNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Regularisation.</span></span><span class="sig-name descname"><span class="pre">InstanceNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_InstanceNormalisation.html#InstanceNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.InstanceNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#DLL.DeepLearning.Layers.Regularisation.GroupNorm" title="DLL.DeepLearning.Layers.Regularisation._GroupNormalisation.GroupNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">GroupNorm</span></code></a></p>
<p>The instance normalisation layer for neural networks. Computes the group norm of a batch along axis=1 with the same number of groups as channels.</p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.InstanceNorm.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_InstanceNormalisation.html#InstanceNorm.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.InstanceNorm.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, channels, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.InstanceNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_InstanceNormalisation.html#InstanceNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.InstanceNorm.forward" title="Link to this definition"></a></dt>
<dd><p>Normalises the input to have zero mean and one variance accross the channel dimension with the following equation:</p>
<div class="math notranslate nohighlight">
\[y = \gamma\frac{x - \mathbb{E}[x]}{\sqrt{\text{var}(x) + \epsilon}} + \beta,\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(\mathbb{E}[x]\)</span> is the expected value or the mean accross the channel dimension, <span class="math notranslate nohighlight">\(\text{var}(x)\)</span> is the variance accross the variance accross the channel dimension, <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are trainable parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>channels</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the normalisation with the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="layernormalisation">
<h2>LayerNormalisation<a class="headerlink" href="#layernormalisation" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.LayerNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DLL.DeepLearning.Layers.Regularisation.</span></span><span class="sig-name descname"><span class="pre">LayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_LayerNormalisation.html#LayerNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.LayerNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#DLL.DeepLearning.Layers.Regularisation.GroupNorm" title="DLL.DeepLearning.Layers.Regularisation._GroupNormalisation.GroupNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">GroupNorm</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.LayerNorm.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dCdy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_LayerNormalisation.html#LayerNorm.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.LayerNorm.backward" title="Link to this definition"></a></dt>
<dd><p>Calculates the gradient of the loss function with respect to the input of the layer. Also calculates the gradients of the loss function with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dCdy</strong> (<em>torch.Tensor</em><em> of </em><em>the same shape as returned from the forward method</em>) – The gradient given by the next layer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The new gradient after backpropagation through the layer.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor of shape (batch_size, channels, …)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="DLL.DeepLearning.Layers.Regularisation.LayerNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/DLL/DeepLearning/Layers/Regularisation/_LayerNormalisation.html#LayerNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#DLL.DeepLearning.Layers.Regularisation.LayerNorm.forward" title="Link to this definition"></a></dt>
<dd><p>Normalises the input to have zero mean and one variance accross the channel dimension with the following equation:</p>
<div class="math notranslate nohighlight">
\[y = \gamma\frac{x - \mathbb{E}[x]}{\sqrt{\text{var}(x) + \epsilon}} + \beta,\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(\mathbb{E}[x]\)</span> is the expected value or the mean accross the channel dimension, <span class="math notranslate nohighlight">\(\text{var}(x)\)</span> is the variance accross the variance accross the channel dimension, <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are trainable parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em><em> of </em><em>shape</em><em> (</em><em>batch_size</em><em>, </em><em>channels</em><em>, </em><em>...</em><em>)</em>) – The input to the layer. Must be a torch.Tensor of the spesified shape given by layer.input_shape.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor after the normalisation with the same shape as the input.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="DLL.DeepLearning.html" class="btn btn-neutral float-left" title="Deep learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="DLL.DeepLearning.Losses.html" class="btn btn-neutral float-right" title="Losses" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Aatu Selkee.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>